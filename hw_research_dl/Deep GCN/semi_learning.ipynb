{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from train import train\n",
    "from train import validate\n",
    "from train import test\n",
    "from utils import load_data\n",
    "from utils import plot_loss_with_acc\n",
    "from utils import accuracy\n",
    "from model import GCN\n",
    "# from odel import SGC\n",
    "from utils import anchor_point\n",
    "from utils import split_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 超参数定义\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DACAY = 5e-4\n",
    "EPOCHS = 1500\n",
    "seed = 42\n",
    "patience = 20\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### load cite data. 加载引文数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "node_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor_adjacency, node_feature, tensor_x, tensor_y, train_mask, val_mask, test_mask, graph, _ = load_data(dataset_str=\"citeseer\")\n",
    "kl_point_citeseer = torch.FloatTensor(anchor_point(graph, tensor_y.numpy()))\n",
    "# num_nodes, input_dim = node_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load coauthor data. 加载coauthor数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dgl.data import CoauthorCSDataset\n",
    "coauthor = CoauthorCSDataset()\n",
    "csd_g = coauthor[0]\n",
    "num_class = coauthor.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "from utils import sys_normalized_adjacency\n",
    "from utils import sparse_mx_to_torch_sparse_tensor\n",
    "from utils import normalize\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "nx_csd = csd_g.to_networkx()\n",
    "\n",
    "adj = sp.coo_matrix(nx.to_numpy_matrix(nx_csd))\n",
    "sys_adj = sys_normalized_adjacency(adj)\n",
    "# 获取稀疏邻接矩阵\n",
    "tensor_adjacency = sparse_mx_to_torch_sparse_tensor(sys_adj)\n",
    "\n",
    "tensor_x = csd_g.ndata['feat']  # get node feature\n",
    "tensor_x = torch.FloatTensor(normalize(tensor_x))\n",
    "tensor_y = csd_g.ndata['label']  # get node labels\n",
    "kl_point = torch.FloatTensor(anchor_point(nx_csd, tensor_y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "# name = \"coauthor\"\n",
    "name = \"amzazon\"\n",
    "torch.save(tensor_x, \"data/{}_x.pt\".format(name))\n",
    "torch.save(tensor_y, \"data/{}_y.pt\".format(name))\n",
    "torch.save(tensor_adjacency, \"data/{}_adj.pt\".format(name))\n",
    "torch.save(kl_point, \"data/{}_kl_point.pt\".format(name))\n",
    "# y = torch.load(\"./SAVE.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semi split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary conserving label-index_numpy, and then randomly select 20 nodes from each class, finally concat\n",
    "num_class = coauthor.num_classes\n",
    "label_dic = {}\n",
    "for i in range(tensor_y.max()+1):\n",
    "    label_dic[i] = []\n",
    "# label_dic\n",
    "for j in range(len(tensor_y)):\n",
    "    label_dic[int(tensor_y[j])].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n",
      "2107\n",
      "1412\n",
      "528\n",
      "5005\n",
      "301\n",
      "483\n",
      "805\n",
      "2113\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "for value in label_dic.values():\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "sample_list = []\n",
    "for i in label_dic:\n",
    "    result = sample(label_dic[i], 20)\n",
    "    sample_list += result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   70,    76,   202,   267,   360,   439,   469,   647,   798,\n",
       "         804,   817,  1021,  1029,  1157,  1180,  1291,  1338,  1418,\n",
       "        1429,  1464,  1494,  1500,  1514,  1516,  1536,  1552,  1584,\n",
       "        1639,  1640,  1665,  1677,  1679,  1703,  1722,  1742,  1759,\n",
       "        1831,  1858,  1886,  1928,  1972,  2008,  2024,  2074,  2076,\n",
       "        2086,  2097,  2105,  2117,  2185,  2186,  2188,  2217,  2256,\n",
       "        2268,  2272,  2350,  2351,  2432,  2504,  2597,  2688,  2726,\n",
       "        2732,  2737,  2798,  2888,  2966,  3134,  3172,  3205,  3275,\n",
       "        3339,  3402,  3415,  3479,  3503,  3597,  3613,  3625,  3692,\n",
       "        3753,  3874,  4161,  4165,  4174,  4227,  4250,  4448,  4458,\n",
       "        4565,  4662,  4708,  4783,  4871,  4964,  5020,  5025,  5033,\n",
       "        5060,  5209,  5339,  5504,  5541,  5726,  6134,  6308,  6406,\n",
       "        6452,  6643,  6646,  6693,  7258,  7322,  7701,  7810,  7914,\n",
       "        8302,  8416,  8606,  8619,  8655,  8688,  8695,  8714,  8754,\n",
       "        8755,  9284,  9285,  9371,  9401,  9419,  9456,  9633,  9662,\n",
       "        9671,  9693,  9801,  9864,  9912,  9930, 10046, 10139, 10195,\n",
       "       10297, 10332, 10450, 10471, 10500, 10549, 10553, 10555, 10648,\n",
       "       10685, 10733, 10779, 10835, 10839, 10865, 10876, 10894, 11080,\n",
       "       11149, 11234, 11274, 11294, 11341, 11379, 11394, 11641, 11673,\n",
       "       11730, 11766, 11798, 11820, 11856, 11857, 12080, 12293, 12460,\n",
       "       12491, 12575, 12581, 12588, 12676, 12697, 12706, 12740, 12816,\n",
       "       12871, 12913, 12970, 13026, 13111, 13148, 13224, 13282, 13287,\n",
       "       13309, 13393])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_list.sort()\n",
    "idx_train = np.array(sample_list)\n",
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_total = list(range(len(tensor_y)))\n",
    "val_test = set_(index_total, sample_list, how=\"-\")\n",
    "# len(val_test)\n",
    "# select 500 for valitation\n",
    "val_list = sample(val_test, 500)\n",
    "test_ = set_(val_test, val_list, how=\"-\")\n",
    "test_list = sample(test_, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nload dataset\\nz = np.load(\\'split_semi/semi_idx_{}.npz\\'.format(name))\\nz[\"idx_train\"]\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "name = \"amazon\"\n",
    "idx_val = np.array(val_list)\n",
    "idx_test = np.array(test_list)\n",
    "np.savez('split_semi/semi_idx_{}'.format(name), idx_train=idx_train, idx_val=idx_val, idx_test=idx_test)\n",
    "\n",
    "\"\"\"\n",
    "load dataset\n",
    "z = np.load('split_semi/semi_idx_{}.npz'.format(name))\n",
    "z[\"idx_train\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_(l1, l2, how):\n",
    "    \"\"\"返回列表，&交，|并，差\"\"\"\n",
    "    set1 = set(l1)\n",
    "    set2 = set(l2)\n",
    "    if how == \"&\":\n",
    "        return(list(set1&set2))\n",
    "    elif how == \"|\":\n",
    "        return(list(set1|set2))\n",
    "    elif how == \"-\":\n",
    "        return(list(set1-set2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 加载amazon数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dgl.data import AmazonCoBuyComputerDataset\n",
    "amazon = AmazonCoBuyComputerDataset()\n",
    "amazon_graph = amazon[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 删除孤立点\n",
    "index = range(amazon_graph.num_nodes())\n",
    "degree = amazon_graph.in_degrees(index)\n",
    "acnode = []\n",
    "for i in range(len(degree)):\n",
    "    if degree[i] == 0:\n",
    "        acnode.append(i)\n",
    "amazon_graph.remove_nodes(acnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "from utils import sys_normalized_adjacency\n",
    "from utils import sparse_mx_to_torch_sparse_tensor\n",
    "from utils import normalize\n",
    "nx_amazon = amazon_graph.to_networkx()\n",
    "adj = sp.coo_matrix(nx.to_numpy_matrix(nx_amazon))\n",
    "sys_adj = sys_normalized_adjacency(adj)\n",
    "# 获取稀疏邻接矩阵\n",
    "tensor_adjacency = sparse_mx_to_torch_sparse_tensor(sys_adj)\n",
    "feat = amazon_graph.ndata['feat']  # get node feature\n",
    "tensor_x = torch.FloatTensor(normalize(feat))\n",
    "tensor_y = amazon_graph.ndata['label']  # get node labels\n",
    "kl_point = torch.FloatTensor(anchor_point(nx_amazon, tensor_y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full split 0.6/0.2/0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "def split_index(num_data, frac_list=None, shuffle=False, random_state=None):\n",
    "    \"\"\"仿照split方法自定义训练集，验证集，测试集的划分\n",
    "    Args:\n",
    "        num_data: int, number of nodes\n",
    "        frac_list:\n",
    "        shuffle:bool, optional\n",
    "        By default we perform a consecutive split of the dataset. If True,\n",
    "        we will first randomly shuffle the dataset.\n",
    "        random_state:None, int or array_like, optional\n",
    "        Random seed used to initialize the pseudo-random number generator.\n",
    "        Can be any integer between 0 and 2**32 - 1 inclusive, an array\n",
    "        (or other sequence) of such integers, or None (the default).\n",
    "        If seed is None, then RandomState will try to read data from /dev/urandom\n",
    "        (or the Windows analogue) if available or seed from the clock otherwise.\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if frac_list is None:\n",
    "        frac_list = [0.8, 0.1, 0.1]\n",
    "    frac_list = np.asarray(frac_list)\n",
    "    lengths = (num_data * frac_list).astype(int)\n",
    "    lengths[-1] = num_data - np.sum(lengths[:-1])\n",
    "    if shuffle:\n",
    "        indices = np.random.RandomState(\n",
    "            seed=random_state).permutation(num_data)\n",
    "    else:\n",
    "        indices = np.arange(num_data)\n",
    "    return [indices[offset - length:offset] for offset, length in zip(accumulate(lengths), lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "name = \"amazon\"\n",
    "idx_val = np.array(val_list)\n",
    "idx_test = np.array(test_list)\n",
    "np.savez('split_semi/semi_idx_{}'.format(name), idx_train=idx_train, idx_val=idx_val, idx_test=idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_adjacency, node_feature, tensor_x, tensor_y, train_mask, val_mask, test_mask, graph, _ = load_data(dataset_str=\"citeseer\")\n",
    "# kl_point_citeseer = torch.FloatTensor(anchor_point(graph, tensor_y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "citeseer_list = split_index(len(tensor_y), frac_list=[0.6, 0.2, 0.2], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"citeseer\"\n",
    "idx_train = citeseer_list[0]\n",
    "idx_val = citeseer_list[1]\n",
    "idx_test = citeseer_list[2]\n",
    "np.savez('split/full_idx_{}'.format(name), idx_train=idx_train, idx_val=idx_val, idx_test=idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 比较各数据集的平均kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "citeseer_a = sum(kl_point_citeseer)/len(kl_point_citeseer)\n",
    "cora_a = sum(kl_point_cora)/len(kl_point_cora)\n",
    "pu_a = sum(kl_point_pubmed)/len(kl_point_pubmed)\n",
    "coauthor_a = sum(kl_point_coauthor)/len(kl_point_coauthor)\n",
    "amazon_a = sum(kl_point_amazon)/len(kl_point_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([kl_point_citeseer.numpy(), kl_point_cora.numpy(), kl_point_pubmed.numpy(),\n",
    "          kl_point_coauthor.numpy(), kl_point_amazon.numpy()],\n",
    "         label=[\"citeseer\",\"cora\", \"pubmed\", \"coauthor\", \"amazon\"],\n",
    "         color = [\"MediumTurquoise\", 'LightSeaGreen', 'DarkCyan', \"Teal\", \"CadetBlue\"],\n",
    "         alpha=1, density=True)\n",
    "plt.xlabel(\"average_ldd\")\n",
    "plt.ylabel(\"density\")\n",
    "#plt.hist(kl_point_cora.numpy(), label=\"cora\", alpha=0.2, density=True)\n",
    "#plt.hist(kl_point_pubmed.numpy(), label=\"pubmed\",  alpha=0.2, density=True)\n",
    "#plt.hist(kl_point_coauthor.numpy(), label=\"coauthor\",  alpha=0.2, density=True)\n",
    "#plt.hist(kl_point_amazon.numpy(), label=\"amazon\",  alpha=0.2, density=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([kl_point_citeseer.numpy(), kl_point_cora.numpy(), kl_point_pubmed.numpy(),\n",
    "          kl_point_coauthor.numpy(), kl_point_amazon.numpy()],\n",
    "         label=[\"citeseer\",\"cora\", \"pubmed\", \"coauthor\", \"amazon\"],\n",
    "         stacked=True,\n",
    "         alpha=1, density=True)\n",
    "plt.xlabel(\"average_ldd\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(citeseer_a, cora_a, pu_a, coauthor_a, amazon_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal_loss\n",
    "import math\n",
    "from utils import penalty_smooth\n",
    "def focal_loss(outputs, y_true, kl_point, w1=0.01,\n",
    "               gamma=2, penalty=False, w_gap=1):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        outputs:\n",
    "        y_true:\n",
    "        kl_point: 训练集需要使用的节点kl，转化为tensor\n",
    "        x:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # 准备损失函数所需数组\n",
    "    probability = F.softmax(outputs, dim=1)\n",
    "    log_p = torch.log(probability)\n",
    "    loss = 0\n",
    "    for i in range(probability.shape[0]):\n",
    "        label_id = y_true[i]\n",
    "        # 当w1=0,gamma=0时，该损失函数转化为交叉熵\n",
    "        loss -= torch.exp(- w1 * kl_point[label_id]) * log_p[i, label_id]\\\n",
    "                * math.pow((1 - probability[i, label_id]), gamma)\n",
    "    if penalty:\n",
    "        # 下面这个函数计算比较慢\n",
    "        gap = penalty_smooth(outputs, y_true)\n",
    "        loss *= torch.tensor(w_gap * gap)\n",
    "    return loss / int(probability.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(10):\\n    for index, data, target in tqdm.tqdm(dataloader):\\n        optimizer.zero_grad()\\n        output = model(data)\\n        loss = criterion(output, target, index)\\n        loss.backward()\\n        optimizer.step()\\n    criterion.increase_threshold()\\n    plot(dataloader.dataset, model, criterion)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spl\n",
    "class SPLLoss(torch.nn.NLLLoss):\n",
    "\n",
    "    def __init__(self, *args, n_samples=0, **kwargs):\n",
    "        super(SPLLoss, self).__init__(*args, **kwargs)\n",
    "        self.threshold = 0.1\n",
    "        self.growing_factor = 1.35\n",
    "        self.v = torch.zeros(n_samples).int()\n",
    "\n",
    "    def forward(self, input, target, index):\n",
    "        pr = F.log_softmax(input, dim=1)\n",
    "        super_loss = F.nll_loss(pr, target, reduction=\"none\")\n",
    "        v = self.spl_loss(super_loss)\n",
    "        self.v[index] = v\n",
    "        return (super_loss * v).mean()\n",
    "\n",
    "    def increase_threshold(self):\n",
    "        self.threshold *= self.growing_factor\n",
    "\n",
    "    def spl_loss(self, super_loss):\n",
    "        v = super_loss < self.threshold\n",
    "        return v.int()\n",
    "\"\"\"\n",
    "for epoch in range(10):\n",
    "    for index, data, target in tqdm.tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target, index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    criterion.increase_threshold()\n",
    "    plot(dataloader.dataset, model, criterion)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from model import GCN\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# 训练框架\n",
    "def train(model, optimizer, loss_f,\n",
    "          tensor_x, tensor_adjacency,\n",
    "          train_mask, tensor_y, kl_point,\n",
    "          alpha=None, w1=0.01, gamma=2,\n",
    "          penalty=False, w_gap=1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "    GCN: feature, adj\n",
    "    \n",
    "    \"\"\"\n",
    "    logits, _ = model(tensor_x, tensor_adjacency, alpha=alpha)\n",
    "    # logits = model(tensor_x, tensor_adjacency) # 自动调用前向传播？\n",
    "    # 损失函数直接在这里定义更改就可\n",
    "    if loss_f == 'focal_loss':\n",
    "        loss_train = focal_loss(logits[train_mask], tensor_y[train_mask].to(device),\n",
    "                                kl_point[train_mask], w1=w1, gamma=gamma,\n",
    "                                penalty=penalty, w_gap=w_gap)\n",
    "    elif loss_f == \"spl\":\n",
    "        criterion = SPLLoss(n_samples=len(tensor_y[train_mask]))\n",
    "        loss_train = criterion(logits[train_mask], tensor_y[train_mask].to(device),\n",
    "                               index=range(len(tensor_y[train_mask])))\n",
    "    else:\n",
    "        # nn对象不能直接调用，需要先声明定义，之后再调用\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss_train = criterion(logits[train_mask], tensor_y[train_mask])\n",
    "    acc_train = accuracy(logits[train_mask], tensor_y[train_mask].to(device))\n",
    "    loss_train.backward()  # 反向传播计算参数的梯度\n",
    "    optimizer.step()  # 使用优化方法进行梯度更新\n",
    "    if loss_f == \"spl\":\n",
    "        criterion.increase_threshold()\n",
    "    return loss_train.item(), acc_train.item()\n",
    "\n",
    "# 验证函数\n",
    "def validate(model, loss_f,\n",
    "             tensor_x, tensor_adjacency,\n",
    "             val_mask, tensor_y, kl_point,\n",
    "             alpha=None, w1=0.01, gamma=2,\n",
    "             penalty=False, w_gap=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 这个特征这里的错误可他妈坑死我了\n",
    "        logits, _ = model(tensor_x, tensor_adjacency, alpha=alpha)\n",
    "        # logits = model(tensor_x, tensor_adjacency)\n",
    "        if loss_f == 'focal_loss':\n",
    "            loss_val = focal_loss(logits[val_mask], tensor_y[val_mask].to(device),\n",
    "                                  kl_point[val_mask], w1=w1, gamma=gamma, \n",
    "                                  penalty=penalty, w_gap=w_gap)\n",
    "        elif loss_f == \"spl\":\n",
    "            criterion = SPLLoss(n_samples=len(tensor_y[val_mask]))\n",
    "            loss_val = criterion(logits[val_mask], tensor_y[val_mask].to(device),\n",
    "                                   index=range(len(tensor_y[val_mask])))\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss_val = criterion(logits[val_mask], tensor_y[val_mask])\n",
    "        acc_val = accuracy(logits[val_mask], tensor_y[val_mask].to(device))\n",
    "        return loss_val.item(), acc_val.item()\n",
    "\n",
    "# 测试函数\n",
    "def test(model, best_model, tensor_x, tensor_adjacency, test_mask, tensor_y, alpha=None):\n",
    "    # 使用在验证集上最好的模型进行测试\n",
    "    model.load_state_dict(torch.load(best_model))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, x_repr = model(tensor_x, tensor_adjacency, alpha=alpha)\n",
    "        # logits = model(tensor_x, tensor_adjacency) # for gat\n",
    "        test_mask_logits = logits[test_mask]\n",
    "        predict_y = test_mask_logits.max(1)[1]\n",
    "        accuarcy = torch.eq(predict_y, tensor_y[test_mask]).float().mean()\n",
    "    return accuarcy, test_mask_logits.cpu().numpy(), tensor_y[test_mask].cpu().numpy(), x_repr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "GCN训练最优参数：\n",
    "python -u train.py --data cora --layer 64 --test\n",
    "python -u train.py --data cora --layer 64 --variant --test\n",
    "python -u train.py --data citeseer --layer 32 --hidden 256 --lamda 0.6 --dropout 0.7 --test\n",
    "python -u train.py --data citeseer --layer 32 --hidden 256 --lamda 0.6 --dropout 0.7 --variant --test\n",
    "python -u train.py --data pubmed --layer 16 --hidden 256 --lamda 0.4 --dropout 0.5 --wd1 5e-4 --test\n",
    "python -u train.py --data pubmed --layer 16 --hidden 256 --lamda 0.4 --dropout 0.5 --wd1 5e-4 --variant --test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cite data\n",
    "data = 'cora'\n",
    "tensor_adjacency, node_feature, tensor_x, tensor_y, train_mask, val_mask, test_mask, graph, _ = load_data(dataset_str=data)\n",
    "kl_point = torch.FloatTensor(anchor_point(graph, tensor_y.numpy()))\n",
    "# kl_point = torch.load(\"data/{}_kl_point.pt\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7fef98570070>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'coo_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m#,with_labels=True)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nx.draw(F,with_labels=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GraphNeuralNetwork-master/lib/python3.10/site-packages/networkx/drawing/nx_pylab.py:121\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    119\u001b[0m     kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds\n\u001b[0;32m--> 121\u001b[0m \u001b[43mdraw_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_axis_off()\n\u001b[1;32m    123\u001b[0m plt\u001b[38;5;241m.\u001b[39mdraw_if_interactive()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GraphNeuralNetwork-master/lib/python3.10/site-packages/networkx/drawing/nx_pylab.py:301\u001b[0m, in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    298\u001b[0m label_kwds \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwds\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m valid_label_kwds}\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# default to spring layout\u001b[39;00m\n\u001b[1;32m    303\u001b[0m draw_networkx_nodes(G, pos, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnode_kwds)\n\u001b[1;32m    304\u001b[0m draw_networkx_edges(G, pos, arrows\u001b[38;5;241m=\u001b[39marrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39medge_kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GraphNeuralNetwork-master/lib/python3.10/site-packages/networkx/utils/decorators.py:845\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[0m, in \u001b[0;36margmap_spring_layout_5\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GraphNeuralNetwork-master/lib/python3.10/site-packages/networkx/drawing/layout.py:476\u001b[0m, in \u001b[0;36mspring_layout\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(G) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:  \u001b[38;5;66;03m# sparse solver for large graphs\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scipy_sparse_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fixed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# We must adjust k by domain size for layouts not near 1x1\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     nnodes, _ \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GraphNeuralNetwork-master/lib/python3.10/site-packages/networkx/convert_matrix.py:923\u001b[0m, in \u001b[0;36mto_scipy_sparse_array\u001b[0;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[1;32m    921\u001b[0m         r \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diag_index\n\u001b[1;32m    922\u001b[0m         c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diag_index\n\u001b[0;32m--> 923\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoo_array\u001b[49m((d, (r, c)), shape\u001b[38;5;241m=\u001b[39m(nlen, nlen), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39masformat(\u001b[38;5;28mformat\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'coo_array'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "nx.draw(graph, node_size=150,width=0.8)    #,with_labels=True)\n",
    "G = nx.from_numpy_matrix(np.matrix(adj_mtx_np), create_using=nx.DiGraph)\n",
    "# nx.draw(F,with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_mask\n",
    "new_idx = np.load(\"split/semi_new_idx_{}.npz\".format(data))\n",
    "idx_train = new_idx[\"idx_train\"]\n",
    "idx_test = new_idx[\"idx_test\"]\n",
    "idx_val = new_idx[\"idx_val\"]\n",
    "\n",
    "train_mask = torch.BoolTensor(sample_mask(idx_train, len(tensor_y)))\n",
    "val_mask = torch.BoolTensor(sample_mask(idx_val, len(tensor_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DACAY = 5e-4\n",
    "EPOCHS = 1500\n",
    "seed = 42\n",
    "patience = 20\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(seed)\n",
    "nhidden = 128\n",
    "dropout = 0.6\n",
    "# penalty = False\n",
    "model_name = 'g1015_GCNII_128_'\n",
    "checkpt_file = 'pretrained/best_' + data + model_name +'.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 构造抽样的测试集\n",
    "from utils import sample_graph_metric\n",
    "from utils import graph_smooth_metric\n",
    "test_dic = sample_graph_metric(len(tensor_y))\n",
    "graph_smooth_metric(test_dic, x_repr[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 原GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GCN\n",
    "from model import GAT\n",
    "from model import APPNP\n",
    "from model import GCNII\n",
    "from model import JKNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w1 = 0.01\n",
    "gamma = 2\n",
    "a = 0.1\n",
    "#lamda = 0.5 \n",
    "dropout = 0.6\n",
    "nhidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN\n",
    "model = GCN(nfeat=tensor_x.shape[1], nclass=int(tensor_y.max()) + 1,\n",
    "            nlayers=nlayer, nhidden=nhidden, dropout=dropout).to(device)\n",
    "        \n",
    "GAT\n",
    "model = GAT(nfeat=tensor_x.shape[1], nhid=nhidden, nclass=int(tensor_y.max()) + 1,\n",
    "            dropout=dropout, alpha=0.2, nheads=nlayer).to(device)\n",
    "\n",
    "APPNP\n",
    "model = APPNP(nfeat=tensor_x.shape[1], nclass=int(tensor_y.max()) + 1,\n",
    "            nlayers=nlayer, nhidden=nhidden, dropout=dropout).to(device)\n",
    "            \n",
    "GCNII\n",
    "model = GCNII(nfeat=tensor_x.shape[1], nlayers=nlayer,\n",
    "              nhidden=nhidden, nclass=int(tensor_y.max()) + 1,\n",
    "              dropout=dropout, lamda=lamda, alpha=a, variant=False).to(device)\n",
    "\n",
    "JKNet\n",
    "model = JKNet(nfeat=tensor_x.shape[1], nhid=nhidden, nclass=int(tensor_y.max()) + 1,\n",
    "        model = JKNet(in_dim=tensor_x.shape[1], hid_dim=nhidden, out_dim=int(tensor_y.max()) + 1,\n",
    "                      num_layers=nlayer, mode=\"lstm\", dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001 train loss:1.110 acc:25.0000 | val loss:1.073 acc:41.2000\n",
      "Epoch:0002 train loss:1.080 acc:38.3333 | val loss:1.043 acc:41.2000\n",
      "Epoch:0003 train loss:1.055 acc:49.1667 | val loss:1.016 acc:56.4000\n",
      "Epoch:0004 train loss:1.006 acc:55.8333 | val loss:0.977 acc:59.6000\n",
      "Epoch:0005 train loss:0.918 acc:70.0000 | val loss:0.919 acc:73.4000\n",
      "Epoch:0006 train loss:0.831 acc:85.0000 | val loss:0.824 acc:73.8000\n",
      "Epoch:0007 train loss:0.662 acc:90.0000 | val loss:0.715 acc:74.2000\n",
      "Epoch:0008 train loss:0.471 acc:93.3333 | val loss:0.654 acc:73.2000\n",
      "Epoch:0009 train loss:0.329 acc:93.3333 | val loss:0.617 acc:74.6000\n",
      "Epoch:0010 train loss:0.209 acc:95.0000 | val loss:0.644 acc:74.0000\n",
      "Epoch:0011 train loss:0.125 acc:96.6667 | val loss:0.751 acc:75.8000\n",
      "Epoch:0012 train loss:0.063 acc:99.1667 | val loss:0.830 acc:76.0000\n",
      "Epoch:0013 train loss:0.036 acc:99.1667 | val loss:0.930 acc:77.8000\n",
      "Epoch:0014 train loss:0.016 acc:100.0000 | val loss:1.094 acc:77.4000\n",
      "Epoch:0015 train loss:0.002 acc:100.0000 | val loss:1.300 acc:78.8000\n",
      "Epoch:0016 train loss:0.002 acc:100.0000 | val loss:1.430 acc:79.0000\n",
      "Epoch:0017 train loss:0.000 acc:100.0000 | val loss:1.555 acc:80.4000\n",
      "Epoch:0018 train loss:0.000 acc:100.0000 | val loss:1.672 acc:80.2000\n",
      "Epoch:0019 train loss:0.000 acc:100.0000 | val loss:1.774 acc:80.6000\n",
      "Epoch:0020 train loss:0.000 acc:100.0000 | val loss:1.856 acc:80.6000\n",
      "Epoch:0021 train loss:0.000 acc:100.0000 | val loss:1.918 acc:81.2000\n",
      "Epoch:0022 train loss:0.000 acc:100.0000 | val loss:1.959 acc:81.4000\n",
      "Epoch:0023 train loss:0.000 acc:100.0000 | val loss:1.980 acc:81.8000\n",
      "Epoch:0024 train loss:0.000 acc:100.0000 | val loss:1.981 acc:81.8000\n",
      "Epoch:0025 train loss:0.000 acc:100.0000 | val loss:1.965 acc:81.6000\n",
      "Epoch:0026 train loss:0.000 acc:100.0000 | val loss:1.930 acc:81.6000\n",
      "Epoch:0027 train loss:0.000 acc:100.0000 | val loss:1.885 acc:81.8000\n",
      "Epoch:0028 train loss:0.000 acc:100.0000 | val loss:1.828 acc:82.0000\n",
      "Epoch:0029 train loss:0.019 acc:99.1667 | val loss:1.821 acc:81.4000\n",
      "Epoch:0030 train loss:0.003 acc:100.0000 | val loss:1.888 acc:79.6000\n",
      "Epoch:0031 train loss:0.009 acc:100.0000 | val loss:1.684 acc:81.4000\n",
      "Epoch:0032 train loss:0.001 acc:100.0000 | val loss:1.566 acc:82.4000\n",
      "Epoch:0033 train loss:0.000 acc:100.0000 | val loss:1.535 acc:82.8000\n",
      "Epoch:0034 train loss:0.000 acc:100.0000 | val loss:1.546 acc:81.6000\n",
      "Epoch:0035 train loss:0.000 acc:100.0000 | val loss:1.581 acc:80.2000\n",
      "Epoch:0036 train loss:0.001 acc:100.0000 | val loss:1.604 acc:78.6000\n",
      "Epoch:0037 train loss:0.003 acc:100.0000 | val loss:1.519 acc:78.6000\n",
      "Epoch:0038 train loss:0.001 acc:100.0000 | val loss:1.370 acc:79.4000\n",
      "Epoch:0039 train loss:0.002 acc:100.0000 | val loss:1.242 acc:79.8000\n",
      "Epoch:0040 train loss:0.003 acc:100.0000 | val loss:1.130 acc:80.0000\n",
      "Epoch:0041 train loss:0.002 acc:100.0000 | val loss:1.040 acc:80.8000\n",
      "Epoch:0042 train loss:0.002 acc:100.0000 | val loss:0.973 acc:80.0000\n",
      "Epoch:0043 train loss:0.003 acc:100.0000 | val loss:0.933 acc:80.2000\n",
      "Epoch:0044 train loss:0.005 acc:100.0000 | val loss:0.880 acc:79.8000\n",
      "Epoch:0045 train loss:0.005 acc:100.0000 | val loss:0.810 acc:80.8000\n",
      "Epoch:0046 train loss:0.006 acc:100.0000 | val loss:0.778 acc:80.8000\n",
      "Epoch:0047 train loss:0.005 acc:100.0000 | val loss:0.772 acc:79.8000\n",
      "Epoch:0048 train loss:0.016 acc:100.0000 | val loss:0.743 acc:80.2000\n",
      "Epoch:0049 train loss:0.011 acc:100.0000 | val loss:0.710 acc:79.6000\n",
      "Epoch:0050 train loss:0.010 acc:100.0000 | val loss:0.675 acc:81.2000\n",
      "Epoch:0051 train loss:0.011 acc:100.0000 | val loss:0.656 acc:81.8000\n",
      "Epoch:0052 train loss:0.012 acc:100.0000 | val loss:0.655 acc:81.6000\n",
      "Epoch:0053 train loss:0.009 acc:100.0000 | val loss:0.693 acc:79.4000\n",
      "Test accuarcy:  0.8019999861717224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/vf0jxkkx5kq60sh06l46fxym0000gn/T/ipykernel_9330/890761032.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss *= torch.tensor(w_gap * gap)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001 train loss:12.873 acc:32.5000 | val loss:2.777 acc:41.2000\n",
      "Epoch:0002 train loss:7.638 acc:40.0000 | val loss:2.660 acc:41.2000\n",
      "Epoch:0003 train loss:6.390 acc:49.1667 | val loss:4.531 acc:66.4000\n",
      "Epoch:0004 train loss:5.864 acc:65.0000 | val loss:4.148 acc:71.2000\n",
      "Epoch:0005 train loss:5.029 acc:70.0000 | val loss:3.742 acc:67.8000\n",
      "Epoch:0006 train loss:3.732 acc:73.3333 | val loss:3.331 acc:71.2000\n",
      "Epoch:0007 train loss:2.651 acc:80.8333 | val loss:2.888 acc:74.2000\n",
      "Epoch:0008 train loss:0.401 acc:90.0000 | val loss:0.960 acc:73.8000\n",
      "Epoch:0009 train loss:0.282 acc:91.6667 | val loss:1.021 acc:73.4000\n",
      "Epoch:0010 train loss:0.239 acc:94.1667 | val loss:1.098 acc:72.6000\n",
      "Epoch:0011 train loss:0.840 acc:92.5000 | val loss:1.260 acc:74.8000\n",
      "Epoch:0012 train loss:0.092 acc:95.8333 | val loss:4.710 acc:75.4000\n",
      "Epoch:0013 train loss:0.110 acc:93.3333 | val loss:5.692 acc:76.2000\n",
      "Epoch:0014 train loss:0.100 acc:95.8333 | val loss:6.426 acc:76.6000\n",
      "Epoch:0015 train loss:0.059 acc:96.6667 | val loss:7.223 acc:77.4000\n",
      "Epoch:0016 train loss:0.090 acc:95.8333 | val loss:2.407 acc:77.6000\n",
      "Epoch:0017 train loss:0.009 acc:100.0000 | val loss:2.287 acc:75.2000\n",
      "Epoch:0018 train loss:0.020 acc:98.3333 | val loss:2.380 acc:74.4000\n",
      "Epoch:0019 train loss:0.131 acc:95.8333 | val loss:3.522 acc:78.4000\n",
      "Epoch:0020 train loss:0.072 acc:99.1667 | val loss:10.029 acc:75.2000\n",
      "Epoch:0021 train loss:0.125 acc:95.0000 | val loss:10.709 acc:72.8000\n",
      "Epoch:0022 train loss:0.158 acc:91.6667 | val loss:9.027 acc:78.8000\n",
      "Epoch:0023 train loss:0.022 acc:99.1667 | val loss:1.862 acc:80.4000\n",
      "Epoch:0024 train loss:0.003 acc:100.0000 | val loss:1.732 acc:79.2000\n",
      "Epoch:0025 train loss:0.000 acc:100.0000 | val loss:1.809 acc:78.6000\n",
      "Epoch:0026 train loss:0.000 acc:100.0000 | val loss:1.967 acc:75.6000\n",
      "Epoch:0027 train loss:0.020 acc:99.1667 | val loss:2.091 acc:74.0000\n",
      "Epoch:0028 train loss:0.012 acc:97.5000 | val loss:2.253 acc:73.0000\n",
      "Epoch:0029 train loss:0.004 acc:99.1667 | val loss:2.461 acc:72.6000\n",
      "Epoch:0030 train loss:0.007 acc:98.3333 | val loss:2.630 acc:72.2000\n",
      "Epoch:0031 train loss:0.004 acc:99.1667 | val loss:2.843 acc:71.2000\n",
      "Epoch:0032 train loss:0.002 acc:100.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0033 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0034 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0035 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0036 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0037 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0038 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0039 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0040 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0041 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0042 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0043 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Test accuarcy:  0.796999990940094\n",
      "Epoch:0001 train loss:1.109 acc:21.6667 | val loss:1.075 acc:41.2000\n",
      "Epoch:0002 train loss:1.084 acc:39.1667 | val loss:1.056 acc:41.2000\n",
      "Epoch:0003 train loss:1.056 acc:40.0000 | val loss:1.033 acc:48.2000\n",
      "Epoch:0004 train loss:1.026 acc:50.0000 | val loss:1.003 acc:56.4000\n",
      "Epoch:0005 train loss:0.937 acc:72.5000 | val loss:0.911 acc:62.0000\n",
      "Epoch:0006 train loss:0.793 acc:78.3333 | val loss:0.806 acc:69.8000\n",
      "Epoch:0007 train loss:0.540 acc:91.6667 | val loss:0.780 acc:69.6000\n",
      "Epoch:0008 train loss:0.384 acc:89.1667 | val loss:1.212 acc:68.4000\n",
      "Epoch:0009 train loss:0.424 acc:84.1667 | val loss:0.971 acc:73.2000\n",
      "Epoch:0010 train loss:0.121 acc:97.5000 | val loss:1.402 acc:68.8000\n",
      "Epoch:0011 train loss:0.210 acc:91.6667 | val loss:1.452 acc:73.4000\n",
      "Epoch:0012 train loss:0.080 acc:95.8333 | val loss:1.514 acc:75.4000\n",
      "Epoch:0013 train loss:0.056 acc:97.5000 | val loss:1.430 acc:79.8000\n",
      "Epoch:0014 train loss:0.026 acc:99.1667 | val loss:1.692 acc:77.2000\n",
      "Epoch:0015 train loss:0.046 acc:98.3333 | val loss:1.740 acc:79.0000\n",
      "Epoch:0016 train loss:0.031 acc:98.3333 | val loss:1.697 acc:80.4000\n",
      "Epoch:0017 train loss:0.003 acc:100.0000 | val loss:1.803 acc:80.8000\n",
      "Epoch:0018 train loss:0.005 acc:100.0000 | val loss:1.972 acc:80.6000\n",
      "Epoch:0019 train loss:0.000 acc:100.0000 | val loss:2.141 acc:79.6000\n",
      "Epoch:0020 train loss:0.001 acc:100.0000 | val loss:2.295 acc:79.6000\n",
      "Epoch:0021 train loss:0.000 acc:100.0000 | val loss:2.439 acc:78.8000\n",
      "Epoch:0022 train loss:0.001 acc:100.0000 | val loss:2.549 acc:78.6000\n",
      "Epoch:0023 train loss:0.000 acc:100.0000 | val loss:2.636 acc:79.2000\n",
      "Epoch:0024 train loss:0.002 acc:100.0000 | val loss:2.638 acc:79.8000\n",
      "Epoch:0025 train loss:0.030 acc:99.1667 | val loss:2.373 acc:82.0000\n",
      "Epoch:0026 train loss:0.000 acc:100.0000 | val loss:2.371 acc:81.6000\n",
      "Epoch:0027 train loss:0.000 acc:100.0000 | val loss:2.536 acc:78.4000\n",
      "Epoch:0028 train loss:0.006 acc:100.0000 | val loss:2.727 acc:76.4000\n",
      "Epoch:0029 train loss:0.027 acc:99.1667 | val loss:2.541 acc:76.4000\n",
      "Epoch:0030 train loss:0.005 acc:100.0000 | val loss:2.300 acc:78.2000\n",
      "Epoch:0031 train loss:0.002 acc:100.0000 | val loss:2.105 acc:80.6000\n",
      "Epoch:0032 train loss:0.000 acc:100.0000 | val loss:1.971 acc:81.0000\n",
      "Epoch:0033 train loss:0.001 acc:100.0000 | val loss:1.880 acc:81.2000\n",
      "Epoch:0034 train loss:0.000 acc:100.0000 | val loss:1.820 acc:81.2000\n",
      "Epoch:0035 train loss:0.000 acc:100.0000 | val loss:1.790 acc:80.6000\n",
      "Epoch:0036 train loss:0.000 acc:100.0000 | val loss:1.773 acc:80.4000\n",
      "Epoch:0037 train loss:0.003 acc:100.0000 | val loss:1.710 acc:80.8000\n",
      "Epoch:0038 train loss:0.001 acc:100.0000 | val loss:1.639 acc:81.2000\n",
      "Epoch:0039 train loss:0.004 acc:100.0000 | val loss:1.556 acc:81.4000\n",
      "Epoch:0040 train loss:0.002 acc:100.0000 | val loss:1.499 acc:82.0000\n",
      "Epoch:0041 train loss:0.001 acc:100.0000 | val loss:1.461 acc:82.2000\n",
      "Epoch:0042 train loss:0.001 acc:100.0000 | val loss:1.436 acc:82.4000\n",
      "Epoch:0043 train loss:0.007 acc:100.0000 | val loss:1.349 acc:82.4000\n",
      "Epoch:0044 train loss:0.001 acc:100.0000 | val loss:1.270 acc:82.8000\n",
      "Epoch:0045 train loss:0.001 acc:100.0000 | val loss:1.200 acc:82.0000\n",
      "Epoch:0046 train loss:0.002 acc:100.0000 | val loss:1.139 acc:81.8000\n",
      "Epoch:0047 train loss:0.001 acc:100.0000 | val loss:1.085 acc:82.0000\n",
      "Epoch:0048 train loss:0.007 acc:100.0000 | val loss:1.036 acc:81.8000\n",
      "Epoch:0049 train loss:0.005 acc:100.0000 | val loss:0.988 acc:81.8000\n",
      "Epoch:0050 train loss:0.002 acc:100.0000 | val loss:0.945 acc:81.8000\n",
      "Epoch:0051 train loss:0.007 acc:100.0000 | val loss:0.907 acc:81.6000\n",
      "Epoch:0052 train loss:0.004 acc:100.0000 | val loss:0.875 acc:81.4000\n",
      "Epoch:0053 train loss:0.004 acc:100.0000 | val loss:0.847 acc:81.4000\n",
      "Epoch:0054 train loss:0.004 acc:100.0000 | val loss:0.823 acc:81.8000\n",
      "Epoch:0055 train loss:0.004 acc:100.0000 | val loss:0.804 acc:81.8000\n",
      "Epoch:0056 train loss:0.006 acc:100.0000 | val loss:0.791 acc:82.0000\n",
      "Epoch:0057 train loss:0.006 acc:100.0000 | val loss:0.786 acc:81.8000\n",
      "Epoch:0058 train loss:0.008 acc:100.0000 | val loss:0.784 acc:82.0000\n",
      "Epoch:0059 train loss:0.005 acc:100.0000 | val loss:0.783 acc:81.8000\n",
      "Epoch:0060 train loss:0.005 acc:100.0000 | val loss:0.780 acc:82.6000\n",
      "Epoch:0061 train loss:0.006 acc:100.0000 | val loss:0.779 acc:82.4000\n",
      "Epoch:0062 train loss:0.005 acc:100.0000 | val loss:0.780 acc:82.2000\n",
      "Epoch:0063 train loss:0.005 acc:100.0000 | val loss:0.783 acc:82.6000\n",
      "Epoch:0064 train loss:0.004 acc:100.0000 | val loss:0.787 acc:82.4000\n",
      "Test accuarcy:  0.8119999766349792\n",
      "Epoch:0001 train loss:7.638 acc:36.6667 | val loss:2.706 acc:41.2000\n",
      "Epoch:0002 train loss:7.542 acc:39.1667 | val loss:1.364 acc:38.4000\n",
      "Epoch:0003 train loss:1.581 acc:38.3333 | val loss:0.942 acc:40.8000\n",
      "Epoch:0004 train loss:1.076 acc:49.1667 | val loss:4.376 acc:60.8000\n",
      "Epoch:0005 train loss:5.230 acc:69.1667 | val loss:4.074 acc:43.8000\n",
      "Epoch:0006 train loss:4.493 acc:64.1667 | val loss:0.690 acc:64.6000\n",
      "Epoch:0007 train loss:0.564 acc:77.5000 | val loss:0.846 acc:55.8000\n",
      "Epoch:0008 train loss:0.673 acc:66.6667 | val loss:0.829 acc:63.4000\n",
      "Epoch:0009 train loss:0.300 acc:80.0000 | val loss:3.985 acc:71.6000\n",
      "Epoch:0010 train loss:0.227 acc:91.6667 | val loss:5.996 acc:67.6000\n",
      "Epoch:0011 train loss:0.429 acc:83.3333 | val loss:8.388 acc:66.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0012 train loss:0.819 acc:85.0000 | val loss:5.483 acc:75.0000\n",
      "Epoch:0013 train loss:0.074 acc:94.1667 | val loss:1.728 acc:70.4000\n",
      "Epoch:0014 train loss:0.156 acc:90.8333 | val loss:1.652 acc:67.4000\n",
      "Epoch:0015 train loss:0.203 acc:90.0000 | val loss:1.669 acc:70.2000\n",
      "Epoch:0016 train loss:0.180 acc:90.8333 | val loss:1.236 acc:74.6000\n",
      "Epoch:0017 train loss:0.045 acc:96.6667 | val loss:0.933 acc:78.6000\n",
      "Epoch:0018 train loss:0.008 acc:100.0000 | val loss:0.857 acc:76.4000\n",
      "Epoch:0019 train loss:0.006 acc:99.1667 | val loss:0.905 acc:76.2000\n",
      "Epoch:0020 train loss:0.012 acc:97.5000 | val loss:0.944 acc:74.6000\n",
      "Epoch:0021 train loss:0.036 acc:95.8333 | val loss:0.981 acc:73.4000\n",
      "Epoch:0022 train loss:0.033 acc:96.6667 | val loss:0.971 acc:75.0000\n",
      "Epoch:0023 train loss:0.008 acc:98.3333 | val loss:0.963 acc:76.0000\n",
      "Epoch:0024 train loss:0.001 acc:100.0000 | val loss:0.994 acc:76.6000\n",
      "Epoch:0025 train loss:0.007 acc:99.1667 | val loss:1.036 acc:77.8000\n",
      "Epoch:0026 train loss:0.004 acc:100.0000 | val loss:1.088 acc:77.4000\n",
      "Epoch:0027 train loss:0.009 acc:99.1667 | val loss:1.133 acc:77.6000\n",
      "Epoch:0028 train loss:0.011 acc:98.3333 | val loss:1.148 acc:78.0000\n",
      "Epoch:0029 train loss:0.008 acc:100.0000 | val loss:1.148 acc:78.6000\n",
      "Epoch:0030 train loss:0.008 acc:99.1667 | val loss:1.151 acc:78.4000\n",
      "Epoch:0031 train loss:0.007 acc:99.1667 | val loss:1.150 acc:78.8000\n",
      "Epoch:0032 train loss:0.005 acc:99.1667 | val loss:1.150 acc:79.2000\n",
      "Epoch:0033 train loss:0.007 acc:99.1667 | val loss:1.167 acc:79.8000\n",
      "Epoch:0034 train loss:0.019 acc:98.3333 | val loss:1.126 acc:79.6000\n",
      "Epoch:0035 train loss:0.020 acc:98.3333 | val loss:1.049 acc:80.0000\n",
      "Epoch:0036 train loss:0.006 acc:99.1667 | val loss:0.987 acc:80.4000\n",
      "Epoch:0037 train loss:0.002 acc:99.1667 | val loss:0.940 acc:80.4000\n",
      "Epoch:0038 train loss:0.004 acc:99.1667 | val loss:0.894 acc:80.6000\n",
      "Epoch:0039 train loss:0.004 acc:99.1667 | val loss:0.850 acc:81.0000\n",
      "Epoch:0040 train loss:0.003 acc:99.1667 | val loss:0.807 acc:80.6000\n",
      "Epoch:0041 train loss:0.002 acc:100.0000 | val loss:0.770 acc:80.2000\n",
      "Epoch:0042 train loss:0.002 acc:100.0000 | val loss:0.736 acc:79.8000\n",
      "Epoch:0043 train loss:0.002 acc:100.0000 | val loss:0.702 acc:79.6000\n",
      "Epoch:0044 train loss:0.002 acc:100.0000 | val loss:0.672 acc:79.6000\n",
      "Epoch:0045 train loss:0.002 acc:100.0000 | val loss:0.645 acc:79.6000\n",
      "Epoch:0046 train loss:0.002 acc:100.0000 | val loss:0.621 acc:79.8000\n",
      "Epoch:0047 train loss:0.003 acc:100.0000 | val loss:0.599 acc:79.6000\n",
      "Epoch:0048 train loss:0.004 acc:100.0000 | val loss:0.580 acc:79.8000\n",
      "Epoch:0049 train loss:0.005 acc:100.0000 | val loss:0.563 acc:79.6000\n",
      "Epoch:0050 train loss:0.004 acc:100.0000 | val loss:0.546 acc:79.6000\n",
      "Epoch:0051 train loss:0.004 acc:100.0000 | val loss:0.530 acc:79.8000\n",
      "Epoch:0052 train loss:0.005 acc:100.0000 | val loss:0.514 acc:79.8000\n",
      "Epoch:0053 train loss:0.008 acc:100.0000 | val loss:0.500 acc:79.8000\n",
      "Epoch:0054 train loss:0.007 acc:100.0000 | val loss:0.491 acc:80.2000\n",
      "Epoch:0055 train loss:0.005 acc:100.0000 | val loss:0.481 acc:80.4000\n",
      "Epoch:0056 train loss:0.008 acc:100.0000 | val loss:0.470 acc:80.6000\n",
      "Epoch:0057 train loss:0.007 acc:100.0000 | val loss:0.461 acc:80.8000\n",
      "Epoch:0058 train loss:0.006 acc:100.0000 | val loss:0.455 acc:80.8000\n",
      "Epoch:0059 train loss:0.007 acc:100.0000 | val loss:0.448 acc:81.0000\n",
      "Test accuarcy:  0.7919999957084656\n",
      "Epoch:0001 train loss:1.100 acc:36.6667 | val loss:1.073 acc:38.4000\n",
      "Epoch:0002 train loss:1.073 acc:40.8333 | val loss:1.047 acc:43.2000\n",
      "Epoch:0003 train loss:1.039 acc:52.5000 | val loss:1.024 acc:54.6000\n",
      "Epoch:0004 train loss:1.034 acc:44.1667 | val loss:1.038 acc:49.8000\n",
      "Epoch:0005 train loss:0.984 acc:58.3333 | val loss:0.950 acc:47.0000\n",
      "Epoch:0006 train loss:0.853 acc:57.5000 | val loss:0.813 acc:70.6000\n",
      "Epoch:0007 train loss:0.640 acc:90.8333 | val loss:0.682 acc:73.8000\n",
      "Epoch:0008 train loss:0.433 acc:89.1667 | val loss:1.146 acc:62.6000\n",
      "Epoch:0009 train loss:0.513 acc:80.0000 | val loss:1.432 acc:54.0000\n",
      "Epoch:0010 train loss:0.798 acc:69.1667 | val loss:1.132 acc:68.0000\n",
      "Epoch:0011 train loss:0.310 acc:90.8333 | val loss:1.228 acc:70.4000\n",
      "Epoch:0012 train loss:0.375 acc:87.5000 | val loss:1.293 acc:69.4000\n",
      "Epoch:0013 train loss:0.402 acc:89.1667 | val loss:0.896 acc:74.2000\n",
      "Epoch:0014 train loss:0.152 acc:95.0000 | val loss:0.789 acc:74.8000\n",
      "Epoch:0015 train loss:0.157 acc:96.6667 | val loss:0.821 acc:72.6000\n",
      "Epoch:0016 train loss:0.221 acc:90.0000 | val loss:0.701 acc:74.6000\n",
      "Epoch:0017 train loss:0.152 acc:94.1667 | val loss:0.575 acc:78.6000\n",
      "Epoch:0018 train loss:0.107 acc:97.5000 | val loss:0.562 acc:78.6000\n",
      "Epoch:0019 train loss:0.083 acc:100.0000 | val loss:0.688 acc:77.6000\n",
      "Epoch:0020 train loss:0.082 acc:98.3333 | val loss:0.781 acc:75.6000\n",
      "Epoch:0021 train loss:0.072 acc:99.1667 | val loss:0.807 acc:76.4000\n",
      "Epoch:0022 train loss:0.058 acc:99.1667 | val loss:0.858 acc:78.4000\n",
      "Epoch:0023 train loss:0.041 acc:100.0000 | val loss:0.951 acc:79.6000\n",
      "Epoch:0024 train loss:0.033 acc:100.0000 | val loss:1.080 acc:80.2000\n",
      "Epoch:0025 train loss:0.018 acc:100.0000 | val loss:1.236 acc:80.2000\n",
      "Epoch:0026 train loss:0.013 acc:100.0000 | val loss:1.406 acc:79.2000\n",
      "Epoch:0027 train loss:0.014 acc:99.1667 | val loss:1.559 acc:79.4000\n",
      "Epoch:0028 train loss:0.015 acc:99.1667 | val loss:1.690 acc:79.6000\n",
      "Epoch:0029 train loss:0.016 acc:99.1667 | val loss:1.792 acc:80.4000\n",
      "Epoch:0030 train loss:0.010 acc:99.1667 | val loss:1.880 acc:81.0000\n",
      "Epoch:0031 train loss:0.005 acc:100.0000 | val loss:1.963 acc:81.0000\n",
      "Epoch:0032 train loss:0.006 acc:100.0000 | val loss:2.030 acc:81.4000\n",
      "Epoch:0033 train loss:0.002 acc:100.0000 | val loss:2.109 acc:81.0000\n",
      "Epoch:0034 train loss:0.001 acc:100.0000 | val loss:2.195 acc:80.0000\n",
      "Epoch:0035 train loss:0.001 acc:100.0000 | val loss:2.294 acc:78.4000\n",
      "Epoch:0036 train loss:0.000 acc:100.0000 | val loss:2.407 acc:77.6000\n",
      "Epoch:0037 train loss:0.002 acc:100.0000 | val loss:2.520 acc:76.8000\n",
      "Epoch:0038 train loss:0.002 acc:100.0000 | val loss:2.618 acc:76.0000\n",
      "Epoch:0039 train loss:0.001 acc:100.0000 | val loss:2.710 acc:75.6000\n",
      "Epoch:0040 train loss:0.003 acc:100.0000 | val loss:2.758 acc:75.6000\n",
      "Epoch:0041 train loss:0.001 acc:100.0000 | val loss:2.762 acc:75.6000\n",
      "Epoch:0042 train loss:0.005 acc:100.0000 | val loss:2.665 acc:75.6000\n",
      "Epoch:0043 train loss:0.001 acc:100.0000 | val loss:2.582 acc:76.6000\n",
      "Epoch:0044 train loss:0.002 acc:100.0000 | val loss:2.501 acc:77.4000\n",
      "Epoch:0045 train loss:0.001 acc:100.0000 | val loss:2.429 acc:78.4000\n",
      "Epoch:0046 train loss:0.001 acc:100.0000 | val loss:2.362 acc:78.4000\n",
      "Epoch:0047 train loss:0.001 acc:100.0000 | val loss:2.299 acc:79.0000\n",
      "Epoch:0048 train loss:0.000 acc:100.0000 | val loss:2.237 acc:79.6000\n",
      "Epoch:0049 train loss:0.003 acc:100.0000 | val loss:2.141 acc:80.0000\n",
      "Epoch:0050 train loss:0.000 acc:100.0000 | val loss:2.049 acc:80.4000\n",
      "Epoch:0051 train loss:0.000 acc:100.0000 | val loss:1.961 acc:80.6000\n",
      "Epoch:0052 train loss:0.000 acc:100.0000 | val loss:1.877 acc:81.0000\n",
      "Test accuarcy:  0.7990000247955322\n",
      "Epoch:0001 train loss:12.734 acc:33.3333 | val loss:2.789 acc:41.2000\n",
      "Epoch:0002 train loss:7.242 acc:36.6667 | val loss:5.530 acc:30.4000\n",
      "Epoch:0003 train loss:12.720 acc:40.8333 | val loss:1.907 acc:43.4000\n",
      "Epoch:0004 train loss:1.901 acc:50.0000 | val loss:1.602 acc:43.4000\n",
      "Epoch:0005 train loss:1.576 acc:50.0000 | val loss:1.031 acc:57.0000\n",
      "Epoch:0006 train loss:0.700 acc:60.0000 | val loss:0.703 acc:60.2000\n",
      "Epoch:0007 train loss:0.653 acc:76.6667 | val loss:0.728 acc:67.8000\n",
      "Epoch:0008 train loss:0.602 acc:86.6667 | val loss:0.701 acc:68.6000\n",
      "Epoch:0009 train loss:0.446 acc:84.1667 | val loss:0.644 acc:68.4000\n",
      "Epoch:0010 train loss:0.345 acc:86.6667 | val loss:0.465 acc:69.4000\n",
      "Epoch:0011 train loss:0.208 acc:91.6667 | val loss:0.426 acc:71.0000\n",
      "Epoch:0012 train loss:0.131 acc:92.5000 | val loss:0.695 acc:71.4000\n",
      "Epoch:0013 train loss:0.179 acc:89.1667 | val loss:0.632 acc:56.2000\n",
      "Epoch:0014 train loss:0.270 acc:71.6667 | val loss:0.918 acc:73.2000\n",
      "Epoch:0015 train loss:0.112 acc:93.3333 | val loss:7.529 acc:64.4000\n",
      "Epoch:0016 train loss:0.953 acc:80.8333 | val loss:1.120 acc:46.6000\n",
      "Epoch:0017 train loss:0.379 acc:65.0000 | val loss:1.654 acc:37.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0018 train loss:0.878 acc:42.5000 | val loss:0.943 acc:55.6000\n",
      "Epoch:0019 train loss:0.346 acc:66.6667 | val loss:0.507 acc:72.8000\n",
      "Epoch:0020 train loss:0.088 acc:92.5000 | val loss:6.413 acc:64.0000\n",
      "Epoch:0021 train loss:0.325 acc:85.0000 | val loss:1.062 acc:66.0000\n",
      "Epoch:0022 train loss:0.213 acc:85.0000 | val loss:0.629 acc:71.8000\n",
      "Epoch:0023 train loss:0.077 acc:89.1667 | val loss:0.810 acc:72.0000\n",
      "Epoch:0024 train loss:0.126 acc:90.8333 | val loss:1.310 acc:70.6000\n",
      "Epoch:0025 train loss:0.214 acc:88.3333 | val loss:5.241 acc:70.2000\n",
      "Epoch:0026 train loss:0.243 acc:88.3333 | val loss:5.038 acc:69.2000\n",
      "Epoch:0027 train loss:0.346 acc:86.6667 | val loss:1.081 acc:70.2000\n",
      "Epoch:0028 train loss:0.143 acc:90.8333 | val loss:1.018 acc:64.8000\n",
      "Epoch:0029 train loss:0.188 acc:87.5000 | val loss:0.786 acc:69.0000\n",
      "Epoch:0030 train loss:0.092 acc:92.5000 | val loss:0.489 acc:73.6000\n",
      "Epoch:0031 train loss:0.055 acc:97.5000 | val loss:0.418 acc:70.4000\n",
      "Epoch:0032 train loss:0.055 acc:94.1667 | val loss:0.441 acc:67.8000\n",
      "Epoch:0033 train loss:0.080 acc:92.5000 | val loss:0.469 acc:62.8000\n",
      "Epoch:0034 train loss:0.079 acc:89.1667 | val loss:0.459 acc:64.6000\n",
      "Epoch:0035 train loss:0.070 acc:88.3333 | val loss:0.450 acc:69.4000\n",
      "Epoch:0036 train loss:0.069 acc:92.5000 | val loss:0.484 acc:72.2000\n",
      "Epoch:0037 train loss:0.066 acc:96.6667 | val loss:0.557 acc:71.4000\n",
      "Epoch:0038 train loss:0.070 acc:95.0000 | val loss:0.627 acc:70.4000\n",
      "Epoch:0039 train loss:0.088 acc:93.3333 | val loss:0.601 acc:71.8000\n",
      "Epoch:0040 train loss:0.065 acc:95.0000 | val loss:0.559 acc:75.4000\n",
      "Epoch:0041 train loss:0.042 acc:97.5000 | val loss:0.533 acc:76.0000\n",
      "Epoch:0042 train loss:0.030 acc:98.3333 | val loss:0.522 acc:76.6000\n",
      "Epoch:0043 train loss:0.024 acc:100.0000 | val loss:0.524 acc:76.2000\n",
      "Epoch:0044 train loss:0.026 acc:99.1667 | val loss:0.536 acc:76.2000\n",
      "Epoch:0045 train loss:0.021 acc:99.1667 | val loss:0.547 acc:76.4000\n",
      "Epoch:0046 train loss:0.026 acc:98.3333 | val loss:0.555 acc:76.6000\n",
      "Epoch:0047 train loss:0.019 acc:99.1667 | val loss:0.568 acc:77.0000\n",
      "Epoch:0048 train loss:0.019 acc:99.1667 | val loss:0.583 acc:77.4000\n",
      "Epoch:0049 train loss:0.015 acc:100.0000 | val loss:0.599 acc:78.2000\n",
      "Epoch:0050 train loss:0.010 acc:100.0000 | val loss:0.616 acc:78.0000\n",
      "Epoch:0051 train loss:0.012 acc:100.0000 | val loss:0.635 acc:77.0000\n",
      "Epoch:0052 train loss:0.012 acc:100.0000 | val loss:0.651 acc:77.6000\n",
      "Epoch:0053 train loss:0.017 acc:98.3333 | val loss:0.647 acc:77.6000\n",
      "Epoch:0054 train loss:0.014 acc:99.1667 | val loss:0.632 acc:78.0000\n",
      "Epoch:0055 train loss:0.010 acc:100.0000 | val loss:0.615 acc:78.2000\n",
      "Epoch:0056 train loss:0.011 acc:99.1667 | val loss:0.595 acc:78.6000\n",
      "Epoch:0057 train loss:0.009 acc:100.0000 | val loss:0.577 acc:78.6000\n",
      "Epoch:0058 train loss:0.008 acc:100.0000 | val loss:0.562 acc:79.0000\n",
      "Epoch:0059 train loss:0.007 acc:100.0000 | val loss:0.552 acc:78.6000\n",
      "Epoch:0060 train loss:0.006 acc:100.0000 | val loss:0.544 acc:77.6000\n",
      "Epoch:0061 train loss:0.009 acc:99.1667 | val loss:0.541 acc:77.8000\n",
      "Epoch:0062 train loss:0.010 acc:99.1667 | val loss:0.540 acc:78.0000\n",
      "Epoch:0063 train loss:0.006 acc:100.0000 | val loss:0.540 acc:78.2000\n",
      "Epoch:0064 train loss:0.008 acc:100.0000 | val loss:0.539 acc:78.4000\n",
      "Epoch:0065 train loss:0.005 acc:100.0000 | val loss:0.537 acc:78.4000\n",
      "Epoch:0066 train loss:0.007 acc:100.0000 | val loss:0.536 acc:78.4000\n",
      "Epoch:0067 train loss:0.006 acc:100.0000 | val loss:0.534 acc:78.6000\n",
      "Epoch:0068 train loss:0.008 acc:100.0000 | val loss:0.532 acc:78.6000\n",
      "Epoch:0069 train loss:0.008 acc:100.0000 | val loss:0.531 acc:78.8000\n",
      "Epoch:0070 train loss:0.007 acc:100.0000 | val loss:0.530 acc:78.8000\n",
      "Epoch:0071 train loss:0.006 acc:100.0000 | val loss:0.529 acc:78.8000\n",
      "Epoch:0072 train loss:0.006 acc:100.0000 | val loss:0.527 acc:78.6000\n",
      "Epoch:0073 train loss:0.008 acc:100.0000 | val loss:0.524 acc:78.6000\n",
      "Epoch:0074 train loss:0.008 acc:100.0000 | val loss:0.520 acc:78.4000\n",
      "Epoch:0075 train loss:0.008 acc:100.0000 | val loss:0.516 acc:78.4000\n",
      "Epoch:0076 train loss:0.008 acc:100.0000 | val loss:0.511 acc:78.2000\n",
      "Epoch:0077 train loss:0.008 acc:100.0000 | val loss:0.508 acc:78.6000\n",
      "Epoch:0078 train loss:0.007 acc:100.0000 | val loss:0.504 acc:78.4000\n",
      "Test accuarcy:  0.7850000262260437\n",
      "Epoch:0001 train loss:1.095 acc:36.6667 | val loss:1.070 acc:44.8000\n",
      "Epoch:0002 train loss:1.076 acc:38.3333 | val loss:1.056 acc:41.2000\n",
      "Epoch:0003 train loss:1.066 acc:41.6667 | val loss:1.040 acc:54.2000\n",
      "Epoch:0004 train loss:1.032 acc:46.6667 | val loss:1.007 acc:40.6000\n",
      "Epoch:0005 train loss:0.966 acc:49.1667 | val loss:0.984 acc:64.2000\n",
      "Epoch:0006 train loss:0.885 acc:75.8333 | val loss:0.879 acc:54.0000\n",
      "Epoch:0007 train loss:0.707 acc:72.5000 | val loss:0.788 acc:64.6000\n",
      "Epoch:0008 train loss:0.566 acc:80.0000 | val loss:0.739 acc:68.8000\n",
      "Epoch:0009 train loss:0.354 acc:89.1667 | val loss:0.676 acc:73.2000\n",
      "Epoch:0010 train loss:0.235 acc:91.6667 | val loss:0.770 acc:74.6000\n",
      "Epoch:0011 train loss:0.102 acc:96.6667 | val loss:1.178 acc:75.6000\n",
      "Epoch:0012 train loss:0.061 acc:97.5000 | val loss:1.916 acc:74.6000\n",
      "Epoch:0013 train loss:0.054 acc:97.5000 | val loss:5.863 acc:66.0000\n",
      "Epoch:0014 train loss:1.569 acc:87.5000 | val loss:2.241 acc:75.2000\n",
      "Epoch:0015 train loss:0.109 acc:95.0000 | val loss:3.883 acc:55.8000\n",
      "Epoch:0016 train loss:1.438 acc:70.0000 | val loss:3.191 acc:62.4000\n",
      "Epoch:0017 train loss:1.163 acc:80.8333 | val loss:1.155 acc:76.0000\n",
      "Epoch:0018 train loss:0.060 acc:99.1667 | val loss:1.206 acc:65.8000\n",
      "Epoch:0019 train loss:0.250 acc:90.0000 | val loss:1.365 acc:60.8000\n",
      "Epoch:0020 train loss:0.436 acc:85.0000 | val loss:1.050 acc:66.0000\n",
      "Epoch:0021 train loss:0.265 acc:88.3333 | val loss:0.776 acc:72.4000\n",
      "Epoch:0022 train loss:0.172 acc:96.6667 | val loss:0.666 acc:74.4000\n",
      "Epoch:0023 train loss:0.187 acc:97.5000 | val loss:0.628 acc:74.0000\n",
      "Epoch:0024 train loss:0.209 acc:95.0000 | val loss:0.585 acc:73.4000\n",
      "Epoch:0025 train loss:0.195 acc:97.5000 | val loss:0.536 acc:79.2000\n",
      "Epoch:0026 train loss:0.161 acc:99.1667 | val loss:0.505 acc:82.0000\n",
      "Epoch:0027 train loss:0.108 acc:100.0000 | val loss:0.516 acc:83.2000\n",
      "Epoch:0028 train loss:0.077 acc:100.0000 | val loss:0.576 acc:83.4000\n",
      "Epoch:0029 train loss:0.061 acc:100.0000 | val loss:0.675 acc:81.6000\n",
      "Epoch:0030 train loss:0.044 acc:100.0000 | val loss:0.798 acc:80.8000\n",
      "Epoch:0031 train loss:0.054 acc:99.1667 | val loss:0.917 acc:80.6000\n",
      "Epoch:0032 train loss:0.055 acc:99.1667 | val loss:1.014 acc:79.8000\n",
      "Epoch:0033 train loss:0.037 acc:99.1667 | val loss:1.091 acc:80.0000\n",
      "Epoch:0034 train loss:0.033 acc:99.1667 | val loss:1.133 acc:80.6000\n",
      "Epoch:0035 train loss:0.033 acc:99.1667 | val loss:1.191 acc:81.2000\n",
      "Epoch:0036 train loss:0.037 acc:98.3333 | val loss:1.360 acc:81.2000\n",
      "Epoch:0037 train loss:0.016 acc:100.0000 | val loss:1.462 acc:81.6000\n",
      "Epoch:0038 train loss:0.076 acc:96.6667 | val loss:1.266 acc:81.2000\n",
      "Epoch:0039 train loss:0.024 acc:99.1667 | val loss:1.255 acc:79.2000\n",
      "Epoch:0040 train loss:0.015 acc:100.0000 | val loss:1.282 acc:78.6000\n",
      "Epoch:0041 train loss:0.012 acc:100.0000 | val loss:1.302 acc:78.6000\n",
      "Epoch:0042 train loss:0.034 acc:99.1667 | val loss:1.297 acc:80.2000\n",
      "Epoch:0043 train loss:0.007 acc:100.0000 | val loss:1.315 acc:80.8000\n",
      "Epoch:0044 train loss:0.003 acc:100.0000 | val loss:1.349 acc:80.8000\n",
      "Epoch:0045 train loss:0.005 acc:100.0000 | val loss:1.393 acc:80.2000\n",
      "Epoch:0046 train loss:0.001 acc:100.0000 | val loss:1.445 acc:80.6000\n",
      "Epoch:0047 train loss:0.002 acc:100.0000 | val loss:1.502 acc:80.8000\n",
      "Epoch:0048 train loss:0.003 acc:100.0000 | val loss:1.557 acc:81.0000\n",
      "Test accuarcy:  0.8100000023841858\n",
      "Epoch:0001 train loss:12.506 acc:28.3333 | val loss:2.560 acc:38.4000\n",
      "Epoch:0002 train loss:12.106 acc:45.8333 | val loss:1.812 acc:41.2000\n",
      "Epoch:0003 train loss:6.700 acc:41.6667 | val loss:1.374 acc:38.8000\n",
      "Epoch:0004 train loss:6.004 acc:45.8333 | val loss:1.015 acc:41.2000\n",
      "Epoch:0005 train loss:1.013 acc:45.8333 | val loss:0.576 acc:51.8000\n",
      "Epoch:0006 train loss:0.611 acc:63.3333 | val loss:0.759 acc:68.2000\n",
      "Epoch:0007 train loss:0.758 acc:80.8333 | val loss:5.581 acc:44.6000\n",
      "Epoch:0008 train loss:4.276 acc:53.3333 | val loss:9.145 acc:24.0000\n",
      "Epoch:0009 train loss:9.609 acc:32.5000 | val loss:3.407 acc:63.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0010 train loss:0.466 acc:79.1667 | val loss:0.703 acc:52.6000\n",
      "Epoch:0011 train loss:0.414 acc:60.8333 | val loss:0.566 acc:55.2000\n",
      "Epoch:0012 train loss:0.293 acc:60.8333 | val loss:0.478 acc:60.4000\n",
      "Epoch:0013 train loss:0.257 acc:68.3333 | val loss:0.416 acc:60.2000\n",
      "Epoch:0014 train loss:0.284 acc:70.0000 | val loss:0.467 acc:57.2000\n",
      "Epoch:0015 train loss:0.387 acc:72.5000 | val loss:0.504 acc:65.8000\n",
      "Epoch:0016 train loss:0.247 acc:83.3333 | val loss:3.421 acc:70.2000\n",
      "Epoch:0017 train loss:0.405 acc:85.0000 | val loss:2.822 acc:74.8000\n",
      "Epoch:0018 train loss:0.230 acc:92.5000 | val loss:0.539 acc:73.4000\n",
      "Epoch:0019 train loss:0.186 acc:89.1667 | val loss:0.474 acc:74.6000\n",
      "Epoch:0020 train loss:0.099 acc:94.1667 | val loss:3.214 acc:75.8000\n",
      "Epoch:0021 train loss:0.104 acc:93.3333 | val loss:0.749 acc:75.8000\n",
      "Epoch:0022 train loss:0.102 acc:92.5000 | val loss:0.448 acc:73.8000\n",
      "Epoch:0023 train loss:0.058 acc:94.1667 | val loss:0.446 acc:72.4000\n",
      "Epoch:0024 train loss:0.151 acc:89.1667 | val loss:1.035 acc:73.0000\n",
      "Epoch:0025 train loss:0.130 acc:93.3333 | val loss:1.181 acc:75.8000\n",
      "Epoch:0026 train loss:0.188 acc:91.6667 | val loss:0.549 acc:73.8000\n",
      "Epoch:0027 train loss:0.088 acc:90.8333 | val loss:0.803 acc:66.0000\n",
      "Epoch:0028 train loss:0.295 acc:81.6667 | val loss:0.594 acc:77.4000\n",
      "Epoch:0029 train loss:0.039 acc:96.6667 | val loss:1.064 acc:69.2000\n",
      "Epoch:0030 train loss:0.127 acc:91.6667 | val loss:0.853 acc:68.8000\n",
      "Epoch:0031 train loss:0.152 acc:87.5000 | val loss:0.685 acc:75.4000\n",
      "Epoch:0032 train loss:0.064 acc:93.3333 | val loss:3.934 acc:75.8000\n",
      "Epoch:0033 train loss:0.038 acc:97.5000 | val loss:4.232 acc:73.2000\n",
      "Epoch:0034 train loss:0.050 acc:97.5000 | val loss:4.275 acc:74.2000\n",
      "Epoch:0035 train loss:0.082 acc:92.5000 | val loss:0.959 acc:77.4000\n",
      "Epoch:0036 train loss:0.047 acc:95.8333 | val loss:0.912 acc:77.2000\n",
      "Epoch:0037 train loss:0.037 acc:98.3333 | val loss:0.731 acc:76.2000\n",
      "Epoch:0038 train loss:0.047 acc:97.5000 | val loss:0.667 acc:75.8000\n",
      "Epoch:0039 train loss:0.047 acc:96.6667 | val loss:0.574 acc:75.8000\n",
      "Epoch:0040 train loss:0.044 acc:98.3333 | val loss:0.549 acc:76.0000\n",
      "Epoch:0041 train loss:0.048 acc:95.8333 | val loss:0.557 acc:76.2000\n",
      "Epoch:0042 train loss:0.070 acc:96.6667 | val loss:0.517 acc:76.4000\n",
      "Epoch:0043 train loss:0.033 acc:98.3333 | val loss:0.529 acc:76.4000\n",
      "Epoch:0044 train loss:0.024 acc:99.1667 | val loss:0.540 acc:76.2000\n",
      "Epoch:0045 train loss:0.032 acc:97.5000 | val loss:0.548 acc:77.2000\n",
      "Epoch:0046 train loss:0.036 acc:95.0000 | val loss:0.531 acc:77.2000\n",
      "Epoch:0047 train loss:0.020 acc:97.5000 | val loss:0.504 acc:78.2000\n",
      "Epoch:0048 train loss:0.017 acc:99.1667 | val loss:0.461 acc:78.0000\n",
      "Epoch:0049 train loss:0.012 acc:100.0000 | val loss:0.432 acc:78.6000\n",
      "Epoch:0050 train loss:0.016 acc:98.3333 | val loss:0.410 acc:79.4000\n",
      "Epoch:0051 train loss:0.013 acc:98.3333 | val loss:0.395 acc:79.8000\n",
      "Epoch:0052 train loss:0.011 acc:99.1667 | val loss:0.385 acc:80.0000\n",
      "Epoch:0053 train loss:0.011 acc:98.3333 | val loss:0.378 acc:80.6000\n",
      "Epoch:0054 train loss:0.011 acc:99.1667 | val loss:0.374 acc:80.6000\n",
      "Epoch:0055 train loss:0.007 acc:99.1667 | val loss:0.371 acc:80.2000\n",
      "Epoch:0056 train loss:0.012 acc:100.0000 | val loss:0.372 acc:80.0000\n",
      "Epoch:0057 train loss:0.011 acc:100.0000 | val loss:0.375 acc:80.0000\n",
      "Epoch:0058 train loss:0.012 acc:100.0000 | val loss:0.381 acc:81.0000\n",
      "Epoch:0059 train loss:0.011 acc:99.1667 | val loss:0.390 acc:80.2000\n",
      "Epoch:0060 train loss:0.011 acc:99.1667 | val loss:0.400 acc:79.8000\n",
      "Epoch:0061 train loss:0.012 acc:99.1667 | val loss:0.412 acc:79.2000\n",
      "Epoch:0062 train loss:0.011 acc:100.0000 | val loss:0.428 acc:79.4000\n",
      "Epoch:0063 train loss:0.012 acc:99.1667 | val loss:0.433 acc:79.4000\n",
      "Epoch:0064 train loss:0.008 acc:100.0000 | val loss:0.442 acc:79.4000\n",
      "Epoch:0065 train loss:0.011 acc:99.1667 | val loss:0.448 acc:78.8000\n",
      "Epoch:0066 train loss:0.011 acc:100.0000 | val loss:0.451 acc:79.4000\n",
      "Epoch:0067 train loss:0.011 acc:100.0000 | val loss:0.451 acc:79.0000\n",
      "Epoch:0068 train loss:0.009 acc:100.0000 | val loss:0.447 acc:78.8000\n",
      "Epoch:0069 train loss:0.014 acc:100.0000 | val loss:0.440 acc:78.8000\n",
      "Epoch:0070 train loss:0.010 acc:100.0000 | val loss:0.432 acc:78.6000\n",
      "Epoch:0071 train loss:0.008 acc:100.0000 | val loss:0.425 acc:78.6000\n",
      "Epoch:0072 train loss:0.010 acc:100.0000 | val loss:0.417 acc:79.0000\n",
      "Epoch:0073 train loss:0.012 acc:99.1667 | val loss:0.411 acc:79.2000\n",
      "Epoch:0074 train loss:0.008 acc:100.0000 | val loss:0.406 acc:79.8000\n",
      "Epoch:0075 train loss:0.008 acc:100.0000 | val loss:0.404 acc:79.6000\n",
      "Epoch:0076 train loss:0.011 acc:99.1667 | val loss:0.402 acc:79.4000\n",
      "Epoch:0077 train loss:0.009 acc:100.0000 | val loss:0.402 acc:79.4000\n",
      "Epoch:0078 train loss:0.007 acc:100.0000 | val loss:0.402 acc:79.4000\n",
      "Test accuarcy:  0.8050000071525574\n",
      "Epoch:0001 train loss:1.101 acc:32.5000 | val loss:1.065 acc:41.2000\n",
      "Epoch:0002 train loss:1.071 acc:43.3333 | val loss:1.047 acc:41.2000\n",
      "Epoch:0003 train loss:1.053 acc:44.1667 | val loss:1.039 acc:51.8000\n",
      "Epoch:0004 train loss:1.028 acc:47.5000 | val loss:0.996 acc:60.2000\n",
      "Epoch:0005 train loss:0.949 acc:70.0000 | val loss:0.920 acc:45.6000\n",
      "Epoch:0006 train loss:0.811 acc:65.0000 | val loss:1.511 acc:26.4000\n",
      "Epoch:0007 train loss:1.233 acc:37.5000 | val loss:0.866 acc:59.2000\n",
      "Epoch:0008 train loss:0.644 acc:75.8333 | val loss:0.953 acc:60.0000\n",
      "Epoch:0009 train loss:0.744 acc:65.8333 | val loss:0.918 acc:55.6000\n",
      "Epoch:0010 train loss:0.657 acc:66.6667 | val loss:0.870 acc:59.6000\n",
      "Epoch:0011 train loss:0.595 acc:70.0000 | val loss:0.773 acc:66.0000\n",
      "Epoch:0012 train loss:0.378 acc:83.3333 | val loss:0.898 acc:65.0000\n",
      "Epoch:0013 train loss:0.372 acc:88.3333 | val loss:0.800 acc:73.0000\n",
      "Epoch:0014 train loss:0.297 acc:95.8333 | val loss:0.749 acc:74.0000\n",
      "Epoch:0015 train loss:0.169 acc:97.5000 | val loss:0.881 acc:72.6000\n",
      "Epoch:0016 train loss:0.191 acc:92.5000 | val loss:0.838 acc:76.6000\n",
      "Epoch:0017 train loss:0.072 acc:98.3333 | val loss:0.975 acc:77.8000\n",
      "Epoch:0018 train loss:0.063 acc:98.3333 | val loss:1.168 acc:78.6000\n",
      "Epoch:0019 train loss:0.035 acc:100.0000 | val loss:1.337 acc:78.2000\n",
      "Epoch:0020 train loss:0.010 acc:100.0000 | val loss:1.469 acc:78.2000\n",
      "Epoch:0021 train loss:0.006 acc:100.0000 | val loss:1.424 acc:79.2000\n",
      "Epoch:0022 train loss:0.004 acc:100.0000 | val loss:1.519 acc:79.4000\n",
      "Epoch:0023 train loss:0.013 acc:99.1667 | val loss:1.652 acc:80.4000\n",
      "Epoch:0024 train loss:0.028 acc:98.3333 | val loss:2.627 acc:75.2000\n",
      "Epoch:0025 train loss:0.125 acc:98.3333 | val loss:1.841 acc:78.4000\n",
      "Epoch:0026 train loss:0.023 acc:99.1667 | val loss:2.195 acc:73.2000\n",
      "Epoch:0027 train loss:0.159 acc:95.0000 | val loss:4.754 acc:65.2000\n",
      "Epoch:0028 train loss:0.716 acc:88.3333 | val loss:2.371 acc:68.8000\n",
      "Epoch:0029 train loss:0.166 acc:96.6667 | val loss:2.620 acc:64.0000\n",
      "Epoch:0030 train loss:0.354 acc:86.6667 | val loss:1.383 acc:77.4000\n",
      "Epoch:0031 train loss:0.012 acc:100.0000 | val loss:2.415 acc:65.6000\n",
      "Epoch:0032 train loss:0.305 acc:90.0000 | val loss:1.118 acc:78.0000\n",
      "Epoch:0033 train loss:0.015 acc:100.0000 | val loss:1.290 acc:71.8000\n",
      "Epoch:0034 train loss:0.112 acc:95.8333 | val loss:0.909 acc:80.0000\n",
      "Epoch:0035 train loss:0.025 acc:99.1667 | val loss:0.974 acc:76.0000\n",
      "Epoch:0036 train loss:0.024 acc:100.0000 | val loss:0.962 acc:79.6000\n",
      "Epoch:0037 train loss:0.069 acc:96.6667 | val loss:1.293 acc:69.8000\n",
      "Epoch:0038 train loss:0.173 acc:92.5000 | val loss:1.295 acc:70.4000\n",
      "Epoch:0039 train loss:0.166 acc:95.0000 | val loss:0.870 acc:80.4000\n",
      "Epoch:0040 train loss:0.022 acc:99.1667 | val loss:0.866 acc:80.6000\n",
      "Epoch:0041 train loss:0.028 acc:100.0000 | val loss:0.858 acc:80.6000\n",
      "Epoch:0042 train loss:0.020 acc:100.0000 | val loss:0.849 acc:80.8000\n",
      "Epoch:0043 train loss:0.018 acc:100.0000 | val loss:1.054 acc:77.6000\n",
      "Epoch:0044 train loss:0.018 acc:100.0000 | val loss:0.877 acc:81.6000\n",
      "Epoch:0045 train loss:0.019 acc:100.0000 | val loss:0.942 acc:79.6000\n",
      "Epoch:0046 train loss:0.008 acc:100.0000 | val loss:1.101 acc:77.8000\n",
      "Epoch:0047 train loss:0.009 acc:100.0000 | val loss:1.023 acc:78.0000\n",
      "Epoch:0048 train loss:0.009 acc:100.0000 | val loss:1.045 acc:78.8000\n",
      "Epoch:0049 train loss:0.007 acc:100.0000 | val loss:1.132 acc:78.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0050 train loss:0.007 acc:100.0000 | val loss:1.181 acc:78.6000\n",
      "Epoch:0051 train loss:0.005 acc:100.0000 | val loss:1.155 acc:80.0000\n",
      "Epoch:0052 train loss:0.002 acc:100.0000 | val loss:1.313 acc:81.2000\n",
      "Epoch:0053 train loss:0.022 acc:99.1667 | val loss:0.856 acc:77.0000\n",
      "Epoch:0054 train loss:0.072 acc:96.6667 | val loss:1.608 acc:73.0000\n",
      "Epoch:0055 train loss:0.033 acc:100.0000 | val loss:1.650 acc:71.0000\n",
      "Epoch:0056 train loss:0.120 acc:96.6667 | val loss:1.105 acc:77.6000\n",
      "Epoch:0057 train loss:0.010 acc:100.0000 | val loss:1.182 acc:79.0000\n",
      "Epoch:0058 train loss:0.038 acc:99.1667 | val loss:1.200 acc:78.4000\n",
      "Epoch:0059 train loss:0.021 acc:99.1667 | val loss:0.987 acc:79.6000\n",
      "Epoch:0060 train loss:0.010 acc:100.0000 | val loss:0.794 acc:82.4000\n",
      "Epoch:0061 train loss:0.005 acc:100.0000 | val loss:0.731 acc:82.8000\n",
      "Epoch:0062 train loss:0.008 acc:100.0000 | val loss:0.722 acc:82.4000\n",
      "Epoch:0063 train loss:0.011 acc:100.0000 | val loss:1.217 acc:74.2000\n",
      "Epoch:0064 train loss:0.022 acc:99.1667 | val loss:2.089 acc:62.2000\n",
      "Epoch:0065 train loss:0.369 acc:87.5000 | val loss:1.729 acc:74.6000\n",
      "Epoch:0066 train loss:0.221 acc:92.5000 | val loss:2.307 acc:69.4000\n",
      "Epoch:0067 train loss:0.272 acc:90.8333 | val loss:3.567 acc:60.2000\n",
      "Epoch:0068 train loss:0.514 acc:85.0000 | val loss:3.298 acc:69.8000\n",
      "Epoch:0069 train loss:1.177 acc:75.8333 | val loss:15.625 acc:20.4000\n",
      "Epoch:0070 train loss:11.667 acc:29.1667 | val loss:4.321 acc:45.8000\n",
      "Epoch:0071 train loss:1.556 acc:57.5000 | val loss:0.927 acc:77.2000\n",
      "Epoch:0072 train loss:0.054 acc:100.0000 | val loss:1.374 acc:64.0000\n",
      "Epoch:0073 train loss:0.558 acc:79.1667 | val loss:6.507 acc:40.0000\n",
      "Epoch:0074 train loss:5.320 acc:49.1667 | val loss:2.712 acc:55.6000\n",
      "Epoch:0075 train loss:1.758 acc:67.5000 | val loss:5.338 acc:21.4000\n",
      "Epoch:0076 train loss:4.256 acc:28.3333 | val loss:2.203 acc:48.0000\n",
      "Epoch:0077 train loss:2.011 acc:52.5000 | val loss:1.100 acc:47.2000\n",
      "Epoch:0078 train loss:0.838 acc:57.5000 | val loss:1.031 acc:64.0000\n",
      "Epoch:0079 train loss:1.281 acc:70.8333 | val loss:5.471 acc:40.8000\n",
      "Epoch:0080 train loss:5.958 acc:38.3333 | val loss:1.351 acc:42.4000\n",
      "Epoch:0081 train loss:1.275 acc:41.6667 | val loss:99.260 acc:38.4000\n",
      "Test accuarcy:  0.8130000233650208\n",
      "Epoch:0001 train loss:7.498 acc:35.0000 | val loss:2.613 acc:42.0000\n",
      "Epoch:0002 train loss:7.326 acc:48.3333 | val loss:4.976 acc:51.8000\n",
      "Epoch:0003 train loss:7.120 acc:52.5000 | val loss:2.144 acc:41.2000\n",
      "Epoch:0004 train loss:7.149 acc:38.3333 | val loss:0.807 acc:42.6000\n",
      "Epoch:0005 train loss:0.738 acc:59.1667 | val loss:0.686 acc:55.6000\n",
      "Epoch:0006 train loss:5.187 acc:65.0000 | val loss:0.518 acc:43.4000\n",
      "Epoch:0007 train loss:0.429 acc:53.3333 | val loss:0.512 acc:56.8000\n",
      "Epoch:0008 train loss:0.406 acc:63.3333 | val loss:0.517 acc:69.6000\n",
      "Epoch:0009 train loss:0.283 acc:88.3333 | val loss:3.242 acc:70.8000\n",
      "Epoch:0010 train loss:2.389 acc:87.5000 | val loss:6.893 acc:43.0000\n",
      "Epoch:0011 train loss:6.453 acc:51.6667 | val loss:9.214 acc:47.4000\n",
      "Epoch:0012 train loss:0.963 acc:55.8333 | val loss:1.390 acc:44.8000\n",
      "Epoch:0013 train loss:0.494 acc:54.1667 | val loss:0.506 acc:65.6000\n",
      "Epoch:0014 train loss:0.212 acc:92.5000 | val loss:0.857 acc:64.8000\n",
      "Epoch:0015 train loss:2.230 acc:89.1667 | val loss:13.563 acc:24.4000\n",
      "Epoch:0016 train loss:12.513 acc:35.0000 | val loss:0.890 acc:59.6000\n",
      "Epoch:0017 train loss:0.297 acc:69.1667 | val loss:4.751 acc:53.6000\n",
      "Epoch:0018 train loss:0.476 acc:60.8333 | val loss:0.993 acc:58.6000\n",
      "Epoch:0019 train loss:0.440 acc:66.6667 | val loss:0.766 acc:61.6000\n",
      "Epoch:0020 train loss:0.513 acc:70.0000 | val loss:0.689 acc:61.8000\n",
      "Epoch:0021 train loss:0.437 acc:72.5000 | val loss:3.863 acc:60.2000\n",
      "Epoch:0022 train loss:3.798 acc:74.1667 | val loss:5.872 acc:49.4000\n",
      "Epoch:0023 train loss:6.910 acc:54.1667 | val loss:0.851 acc:57.0000\n",
      "Epoch:0024 train loss:0.454 acc:70.0000 | val loss:0.964 acc:57.4000\n",
      "Epoch:0025 train loss:0.573 acc:70.0000 | val loss:0.859 acc:50.2000\n",
      "Epoch:0026 train loss:0.615 acc:60.8333 | val loss:1.450 acc:41.4000\n",
      "Epoch:0027 train loss:0.960 acc:40.0000 | val loss:8.406 acc:41.0000\n",
      "Epoch:0028 train loss:1.666 acc:50.0000 | val loss:22.834 acc:41.2000\n",
      "Epoch:0029 train loss:99.361 acc:38.3333 | val loss:1.058 acc:44.2000\n",
      "Test accuarcy:  0.6980000138282776\n",
      "Epoch:0001 train loss:1.103 acc:29.1667 | val loss:1.067 acc:46.6000\n",
      "Epoch:0002 train loss:1.081 acc:40.8333 | val loss:1.044 acc:43.4000\n",
      "Epoch:0003 train loss:1.048 acc:50.8333 | val loss:1.024 acc:54.6000\n",
      "Epoch:0004 train loss:1.022 acc:50.8333 | val loss:0.996 acc:57.8000\n",
      "Epoch:0005 train loss:0.939 acc:71.6667 | val loss:0.923 acc:57.6000\n",
      "Epoch:0006 train loss:0.808 acc:78.3333 | val loss:0.840 acc:56.2000\n",
      "Epoch:0007 train loss:0.644 acc:71.6667 | val loss:2.008 acc:27.8000\n",
      "Epoch:0008 train loss:1.386 acc:45.8333 | val loss:0.828 acc:58.4000\n",
      "Epoch:0009 train loss:0.466 acc:81.6667 | val loss:1.211 acc:45.2000\n",
      "Epoch:0010 train loss:0.782 acc:58.3333 | val loss:0.846 acc:65.0000\n",
      "Epoch:0011 train loss:0.436 acc:82.5000 | val loss:0.700 acc:70.0000\n",
      "Epoch:0012 train loss:0.292 acc:89.1667 | val loss:0.944 acc:70.2000\n",
      "Epoch:0013 train loss:0.426 acc:89.1667 | val loss:0.980 acc:70.6000\n",
      "Epoch:0014 train loss:0.353 acc:88.3333 | val loss:0.942 acc:71.4000\n",
      "Epoch:0015 train loss:0.339 acc:90.0000 | val loss:0.776 acc:72.4000\n",
      "Epoch:0016 train loss:0.225 acc:95.0000 | val loss:0.727 acc:74.4000\n",
      "Epoch:0017 train loss:0.229 acc:92.5000 | val loss:0.601 acc:78.2000\n",
      "Epoch:0018 train loss:0.103 acc:100.0000 | val loss:0.639 acc:77.6000\n",
      "Epoch:0019 train loss:0.081 acc:99.1667 | val loss:0.729 acc:75.2000\n",
      "Epoch:0020 train loss:0.096 acc:99.1667 | val loss:0.758 acc:75.4000\n",
      "Epoch:0021 train loss:0.074 acc:99.1667 | val loss:0.748 acc:76.8000\n",
      "Epoch:0022 train loss:0.062 acc:100.0000 | val loss:0.757 acc:79.2000\n",
      "Epoch:0023 train loss:0.043 acc:100.0000 | val loss:0.779 acc:79.8000\n",
      "Epoch:0024 train loss:0.029 acc:100.0000 | val loss:0.813 acc:79.6000\n",
      "Epoch:0025 train loss:0.019 acc:100.0000 | val loss:0.865 acc:79.8000\n",
      "Epoch:0026 train loss:0.012 acc:100.0000 | val loss:0.941 acc:79.6000\n",
      "Epoch:0027 train loss:0.010 acc:100.0000 | val loss:1.034 acc:78.6000\n",
      "Epoch:0028 train loss:0.008 acc:100.0000 | val loss:1.141 acc:78.0000\n",
      "Epoch:0029 train loss:0.020 acc:99.1667 | val loss:1.234 acc:77.4000\n",
      "Epoch:0030 train loss:0.015 acc:100.0000 | val loss:1.290 acc:78.0000\n",
      "Epoch:0031 train loss:0.009 acc:100.0000 | val loss:1.319 acc:78.8000\n",
      "Epoch:0032 train loss:0.004 acc:100.0000 | val loss:1.346 acc:79.8000\n",
      "Epoch:0033 train loss:0.006 acc:100.0000 | val loss:1.377 acc:80.6000\n",
      "Epoch:0034 train loss:0.003 acc:100.0000 | val loss:1.417 acc:81.2000\n",
      "Epoch:0035 train loss:0.002 acc:100.0000 | val loss:1.459 acc:80.8000\n",
      "Epoch:0036 train loss:0.002 acc:100.0000 | val loss:1.496 acc:80.8000\n",
      "Epoch:0037 train loss:0.004 acc:100.0000 | val loss:1.513 acc:80.8000\n",
      "Epoch:0038 train loss:0.002 acc:100.0000 | val loss:1.529 acc:80.8000\n",
      "Epoch:0039 train loss:0.001 acc:100.0000 | val loss:1.549 acc:81.0000\n",
      "Epoch:0040 train loss:0.001 acc:100.0000 | val loss:1.575 acc:81.4000\n",
      "Epoch:0041 train loss:0.002 acc:100.0000 | val loss:1.616 acc:80.8000\n",
      "Epoch:0042 train loss:0.000 acc:100.0000 | val loss:1.663 acc:79.8000\n",
      "Epoch:0043 train loss:0.001 acc:100.0000 | val loss:1.721 acc:78.6000\n",
      "Epoch:0044 train loss:0.001 acc:100.0000 | val loss:1.771 acc:77.6000\n",
      "Epoch:0045 train loss:0.003 acc:100.0000 | val loss:1.763 acc:77.6000\n",
      "Epoch:0046 train loss:0.001 acc:100.0000 | val loss:1.735 acc:77.6000\n",
      "Epoch:0047 train loss:0.003 acc:100.0000 | val loss:1.676 acc:79.0000\n",
      "Epoch:0048 train loss:0.001 acc:100.0000 | val loss:1.616 acc:80.0000\n",
      "Epoch:0049 train loss:0.001 acc:100.0000 | val loss:1.555 acc:80.6000\n",
      "Epoch:0050 train loss:0.001 acc:100.0000 | val loss:1.497 acc:81.0000\n",
      "Epoch:0051 train loss:0.002 acc:100.0000 | val loss:1.439 acc:81.0000\n",
      "Epoch:0052 train loss:0.001 acc:100.0000 | val loss:1.388 acc:82.4000\n",
      "Epoch:0053 train loss:0.002 acc:100.0000 | val loss:1.340 acc:81.4000\n",
      "Epoch:0054 train loss:0.002 acc:100.0000 | val loss:1.300 acc:81.4000\n",
      "Epoch:0055 train loss:0.002 acc:100.0000 | val loss:1.263 acc:81.6000\n",
      "Epoch:0056 train loss:0.003 acc:100.0000 | val loss:1.231 acc:81.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0057 train loss:0.001 acc:100.0000 | val loss:1.199 acc:81.8000\n",
      "Epoch:0058 train loss:0.003 acc:100.0000 | val loss:1.170 acc:82.0000\n",
      "Epoch:0059 train loss:0.002 acc:100.0000 | val loss:1.146 acc:82.4000\n",
      "Epoch:0060 train loss:0.002 acc:100.0000 | val loss:1.125 acc:82.4000\n",
      "Epoch:0061 train loss:0.001 acc:100.0000 | val loss:1.107 acc:81.8000\n",
      "Epoch:0062 train loss:0.002 acc:100.0000 | val loss:1.092 acc:81.2000\n",
      "Epoch:0063 train loss:0.003 acc:100.0000 | val loss:1.076 acc:81.0000\n",
      "Epoch:0064 train loss:0.003 acc:100.0000 | val loss:1.062 acc:81.4000\n",
      "Epoch:0065 train loss:0.002 acc:100.0000 | val loss:1.044 acc:81.0000\n",
      "Epoch:0066 train loss:0.005 acc:100.0000 | val loss:1.009 acc:81.0000\n",
      "Epoch:0067 train loss:0.003 acc:100.0000 | val loss:0.976 acc:81.4000\n",
      "Epoch:0068 train loss:0.002 acc:100.0000 | val loss:0.948 acc:82.0000\n",
      "Epoch:0069 train loss:0.006 acc:100.0000 | val loss:0.921 acc:82.4000\n",
      "Epoch:0070 train loss:0.003 acc:100.0000 | val loss:0.900 acc:82.6000\n",
      "Epoch:0071 train loss:0.005 acc:100.0000 | val loss:0.885 acc:82.6000\n",
      "Epoch:0072 train loss:0.005 acc:100.0000 | val loss:0.877 acc:82.4000\n",
      "Epoch:0073 train loss:0.004 acc:100.0000 | val loss:0.871 acc:82.4000\n",
      "Epoch:0074 train loss:0.005 acc:100.0000 | val loss:0.866 acc:82.4000\n",
      "Epoch:0075 train loss:0.005 acc:100.0000 | val loss:0.863 acc:82.2000\n",
      "Epoch:0076 train loss:0.005 acc:100.0000 | val loss:0.859 acc:82.0000\n",
      "Epoch:0077 train loss:0.007 acc:100.0000 | val loss:0.853 acc:82.2000\n",
      "Epoch:0078 train loss:0.006 acc:100.0000 | val loss:0.847 acc:82.2000\n",
      "Epoch:0079 train loss:0.005 acc:100.0000 | val loss:0.838 acc:82.4000\n",
      "Epoch:0080 train loss:0.007 acc:100.0000 | val loss:0.841 acc:82.4000\n",
      "Epoch:0081 train loss:0.005 acc:100.0000 | val loss:0.844 acc:82.4000\n",
      "Epoch:0082 train loss:0.006 acc:100.0000 | val loss:0.849 acc:81.8000\n",
      "Epoch:0083 train loss:0.004 acc:100.0000 | val loss:0.854 acc:81.6000\n",
      "Epoch:0084 train loss:0.005 acc:100.0000 | val loss:0.858 acc:81.8000\n",
      "Epoch:0085 train loss:0.006 acc:100.0000 | val loss:0.860 acc:81.8000\n",
      "Epoch:0086 train loss:0.006 acc:100.0000 | val loss:0.861 acc:82.0000\n",
      "Epoch:0087 train loss:0.004 acc:100.0000 | val loss:0.859 acc:82.2000\n",
      "Epoch:0088 train loss:0.004 acc:100.0000 | val loss:0.861 acc:82.2000\n",
      "Epoch:0089 train loss:0.005 acc:100.0000 | val loss:0.859 acc:82.6000\n",
      "Epoch:0090 train loss:0.004 acc:100.0000 | val loss:0.857 acc:83.2000\n",
      "Epoch:0091 train loss:0.004 acc:100.0000 | val loss:0.859 acc:83.2000\n",
      "Epoch:0092 train loss:0.006 acc:100.0000 | val loss:0.865 acc:83.2000\n",
      "Epoch:0093 train loss:0.005 acc:100.0000 | val loss:0.871 acc:82.8000\n",
      "Epoch:0094 train loss:0.003 acc:100.0000 | val loss:0.877 acc:82.4000\n",
      "Epoch:0095 train loss:0.003 acc:100.0000 | val loss:0.886 acc:82.6000\n",
      "Epoch:0096 train loss:0.004 acc:100.0000 | val loss:0.899 acc:82.2000\n",
      "Epoch:0097 train loss:0.004 acc:100.0000 | val loss:0.907 acc:81.8000\n",
      "Epoch:0098 train loss:0.005 acc:100.0000 | val loss:0.905 acc:81.8000\n",
      "Epoch:0099 train loss:0.003 acc:100.0000 | val loss:0.898 acc:82.4000\n",
      "Epoch:0100 train loss:0.004 acc:100.0000 | val loss:0.894 acc:82.4000\n",
      "Epoch:0101 train loss:0.004 acc:100.0000 | val loss:0.887 acc:82.6000\n",
      "Epoch:0102 train loss:0.003 acc:100.0000 | val loss:0.885 acc:82.6000\n",
      "Epoch:0103 train loss:0.003 acc:100.0000 | val loss:0.881 acc:82.2000\n",
      "Epoch:0104 train loss:0.003 acc:100.0000 | val loss:0.882 acc:82.2000\n",
      "Epoch:0105 train loss:0.003 acc:100.0000 | val loss:0.879 acc:82.2000\n",
      "Epoch:0106 train loss:0.004 acc:100.0000 | val loss:0.877 acc:82.2000\n",
      "Epoch:0107 train loss:0.004 acc:100.0000 | val loss:0.879 acc:82.0000\n",
      "Epoch:0108 train loss:0.004 acc:100.0000 | val loss:0.877 acc:82.2000\n",
      "Epoch:0109 train loss:0.004 acc:100.0000 | val loss:0.878 acc:82.2000\n",
      "Epoch:0110 train loss:0.004 acc:100.0000 | val loss:0.879 acc:82.2000\n",
      "Test accuarcy:  0.8040000200271606\n",
      "Epoch:0001 train loss:12.553 acc:38.3333 | val loss:2.988 acc:38.4000\n",
      "Epoch:0002 train loss:12.649 acc:36.6667 | val loss:5.187 acc:42.0000\n",
      "Epoch:0003 train loss:6.883 acc:39.1667 | val loss:4.884 acc:44.4000\n",
      "Epoch:0004 train loss:6.136 acc:58.3333 | val loss:4.532 acc:49.2000\n",
      "Epoch:0005 train loss:5.306 acc:62.5000 | val loss:7.390 acc:21.8000\n",
      "Epoch:0006 train loss:8.378 acc:27.5000 | val loss:1.873 acc:42.6000\n",
      "Epoch:0007 train loss:1.419 acc:45.0000 | val loss:5.053 acc:42.8000\n",
      "Epoch:0008 train loss:1.326 acc:47.5000 | val loss:3.850 acc:60.8000\n",
      "Epoch:0009 train loss:4.162 acc:72.5000 | val loss:11.412 acc:20.4000\n",
      "Epoch:0010 train loss:32.359 acc:25.8333 | val loss:1.243 acc:58.8000\n",
      "Epoch:0011 train loss:0.883 acc:62.5000 | val loss:0.951 acc:59.8000\n",
      "Epoch:0012 train loss:0.672 acc:61.6667 | val loss:0.833 acc:60.0000\n",
      "Epoch:0013 train loss:0.628 acc:67.5000 | val loss:0.703 acc:62.2000\n",
      "Epoch:0014 train loss:0.501 acc:65.8333 | val loss:0.650 acc:62.6000\n",
      "Epoch:0015 train loss:0.466 acc:70.0000 | val loss:0.717 acc:62.8000\n",
      "Epoch:0016 train loss:0.586 acc:70.8333 | val loss:0.859 acc:62.6000\n",
      "Epoch:0017 train loss:0.993 acc:68.3333 | val loss:0.935 acc:63.0000\n",
      "Epoch:0018 train loss:4.122 acc:74.1667 | val loss:7.035 acc:63.0000\n",
      "Epoch:0019 train loss:4.097 acc:70.8333 | val loss:0.814 acc:62.2000\n",
      "Epoch:0020 train loss:3.699 acc:72.5000 | val loss:3.593 acc:61.4000\n",
      "Epoch:0021 train loss:0.810 acc:70.0000 | val loss:0.815 acc:62.8000\n",
      "Epoch:0022 train loss:0.573 acc:75.0000 | val loss:3.533 acc:64.0000\n",
      "Epoch:0023 train loss:0.538 acc:77.5000 | val loss:3.644 acc:65.6000\n",
      "Epoch:0024 train loss:0.545 acc:76.6667 | val loss:3.591 acc:64.0000\n",
      "Epoch:0025 train loss:0.571 acc:82.5000 | val loss:4.390 acc:69.6000\n",
      "Epoch:0026 train loss:2.255 acc:85.8333 | val loss:9.896 acc:53.2000\n",
      "Epoch:0027 train loss:24.066 acc:50.0000 | val loss:3.303 acc:68.2000\n",
      "Epoch:0028 train loss:1.929 acc:85.8333 | val loss:20.070 acc:27.4000\n",
      "Epoch:0029 train loss:61.674 acc:42.5000 | val loss:1.672 acc:49.8000\n",
      "Epoch:0030 train loss:1.409 acc:57.5000 | val loss:1.933 acc:56.6000\n",
      "Epoch:0031 train loss:3.052 acc:51.6667 | val loss:0.574 acc:59.4000\n",
      "Epoch:0032 train loss:0.990 acc:59.1667 | val loss:1.229 acc:41.0000\n",
      "Epoch:0033 train loss:1.945 acc:42.5000 | val loss:6.327 acc:52.2000\n",
      "Epoch:0034 train loss:7.599 acc:53.3333 | val loss:6.312 acc:22.2000\n",
      "Epoch:0035 train loss:41.507 acc:36.6667 | val loss:inf acc:41.2000\n",
      "Epoch:0036 train loss:inf acc:38.3333 | val loss:nan acc:20.4000\n",
      "Epoch:0037 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0038 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0039 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0040 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0041 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0042 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0043 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0044 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Epoch:0045 train loss:nan acc:25.0000 | val loss:nan acc:20.4000\n",
      "Test accuarcy:  0.6919999718666077\n",
      "Epoch:0001 train loss:1.111 acc:24.1667 | val loss:1.073 acc:38.4000\n",
      "Epoch:0002 train loss:1.080 acc:38.3333 | val loss:1.054 acc:41.2000\n",
      "Epoch:0003 train loss:1.073 acc:40.8333 | val loss:1.037 acc:53.0000\n",
      "Epoch:0004 train loss:1.027 acc:50.0000 | val loss:1.010 acc:57.6000\n",
      "Epoch:0005 train loss:0.961 acc:68.3333 | val loss:0.933 acc:65.8000\n",
      "Epoch:0006 train loss:0.835 acc:90.0000 | val loss:0.842 acc:55.6000\n",
      "Epoch:0007 train loss:0.624 acc:73.3333 | val loss:2.660 acc:24.6000\n",
      "Epoch:0008 train loss:1.896 acc:35.8333 | val loss:0.858 acc:58.8000\n",
      "Epoch:0009 train loss:0.501 acc:78.3333 | val loss:1.152 acc:56.6000\n",
      "Epoch:0010 train loss:0.873 acc:60.0000 | val loss:0.998 acc:59.0000\n",
      "Epoch:0011 train loss:0.690 acc:69.1667 | val loss:0.834 acc:62.0000\n",
      "Epoch:0012 train loss:0.516 acc:73.3333 | val loss:0.799 acc:63.8000\n",
      "Epoch:0013 train loss:0.482 acc:75.0000 | val loss:0.891 acc:64.8000\n",
      "Epoch:0014 train loss:0.422 acc:81.6667 | val loss:0.778 acc:72.6000\n",
      "Epoch:0015 train loss:0.313 acc:94.1667 | val loss:0.802 acc:73.0000\n",
      "Epoch:0016 train loss:0.283 acc:92.5000 | val loss:0.896 acc:68.0000\n",
      "Epoch:0017 train loss:0.398 acc:84.1667 | val loss:1.154 acc:66.8000\n",
      "Epoch:0018 train loss:0.422 acc:85.0000 | val loss:0.852 acc:73.8000\n",
      "Epoch:0019 train loss:0.155 acc:95.8333 | val loss:1.226 acc:69.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0020 train loss:0.250 acc:90.8333 | val loss:0.956 acc:75.6000\n",
      "Epoch:0021 train loss:0.116 acc:97.5000 | val loss:0.936 acc:77.4000\n",
      "Epoch:0022 train loss:0.093 acc:97.5000 | val loss:1.000 acc:78.2000\n",
      "Epoch:0023 train loss:0.106 acc:95.8333 | val loss:0.883 acc:80.6000\n",
      "Epoch:0024 train loss:0.037 acc:100.0000 | val loss:0.890 acc:80.2000\n",
      "Epoch:0025 train loss:0.017 acc:100.0000 | val loss:0.955 acc:78.8000\n",
      "Epoch:0026 train loss:0.015 acc:100.0000 | val loss:1.018 acc:78.0000\n",
      "Epoch:0027 train loss:0.017 acc:100.0000 | val loss:1.043 acc:78.2000\n",
      "Epoch:0028 train loss:0.011 acc:100.0000 | val loss:1.053 acc:79.0000\n",
      "Epoch:0029 train loss:0.010 acc:100.0000 | val loss:1.060 acc:80.2000\n",
      "Epoch:0030 train loss:0.021 acc:99.1667 | val loss:1.114 acc:80.0000\n",
      "Epoch:0031 train loss:0.008 acc:100.0000 | val loss:1.187 acc:80.2000\n",
      "Epoch:0032 train loss:0.003 acc:100.0000 | val loss:1.265 acc:80.0000\n",
      "Epoch:0033 train loss:0.002 acc:100.0000 | val loss:1.332 acc:79.8000\n",
      "Epoch:0034 train loss:0.004 acc:100.0000 | val loss:1.347 acc:80.2000\n",
      "Epoch:0035 train loss:0.002 acc:100.0000 | val loss:1.360 acc:81.2000\n",
      "Epoch:0036 train loss:0.002 acc:100.0000 | val loss:1.380 acc:81.6000\n",
      "Epoch:0037 train loss:0.012 acc:99.1667 | val loss:1.517 acc:80.2000\n",
      "Epoch:0038 train loss:0.015 acc:99.1667 | val loss:1.416 acc:82.6000\n",
      "Epoch:0039 train loss:0.000 acc:100.0000 | val loss:1.473 acc:81.2000\n",
      "Epoch:0040 train loss:0.002 acc:100.0000 | val loss:1.515 acc:80.6000\n",
      "Epoch:0041 train loss:0.001 acc:100.0000 | val loss:1.528 acc:80.4000\n",
      "Epoch:0042 train loss:0.001 acc:100.0000 | val loss:1.516 acc:80.8000\n",
      "Epoch:0043 train loss:0.002 acc:100.0000 | val loss:1.493 acc:81.2000\n",
      "Epoch:0044 train loss:0.001 acc:100.0000 | val loss:1.445 acc:81.0000\n",
      "Epoch:0045 train loss:0.001 acc:100.0000 | val loss:1.403 acc:81.6000\n",
      "Epoch:0046 train loss:0.000 acc:100.0000 | val loss:1.370 acc:82.2000\n",
      "Epoch:0047 train loss:0.000 acc:100.0000 | val loss:1.345 acc:83.0000\n",
      "Epoch:0048 train loss:0.000 acc:100.0000 | val loss:1.323 acc:82.6000\n",
      "Epoch:0049 train loss:0.001 acc:100.0000 | val loss:1.302 acc:82.6000\n",
      "Epoch:0050 train loss:0.001 acc:100.0000 | val loss:1.277 acc:82.4000\n",
      "Epoch:0051 train loss:0.001 acc:100.0000 | val loss:1.249 acc:82.4000\n",
      "Epoch:0052 train loss:0.001 acc:100.0000 | val loss:1.223 acc:82.6000\n",
      "Epoch:0053 train loss:0.001 acc:100.0000 | val loss:1.201 acc:83.0000\n",
      "Epoch:0054 train loss:0.002 acc:100.0000 | val loss:1.198 acc:82.0000\n",
      "Epoch:0055 train loss:0.000 acc:100.0000 | val loss:1.206 acc:80.8000\n",
      "Epoch:0056 train loss:0.001 acc:100.0000 | val loss:1.215 acc:81.2000\n",
      "Epoch:0057 train loss:0.001 acc:100.0000 | val loss:1.192 acc:81.2000\n",
      "Epoch:0058 train loss:0.002 acc:100.0000 | val loss:1.150 acc:81.2000\n",
      "Epoch:0059 train loss:0.002 acc:100.0000 | val loss:1.106 acc:81.0000\n",
      "Epoch:0060 train loss:0.001 acc:100.0000 | val loss:1.063 acc:81.0000\n",
      "Epoch:0061 train loss:0.002 acc:100.0000 | val loss:1.023 acc:82.0000\n",
      "Epoch:0062 train loss:0.002 acc:100.0000 | val loss:0.984 acc:82.2000\n",
      "Epoch:0063 train loss:0.002 acc:100.0000 | val loss:0.950 acc:82.4000\n",
      "Epoch:0064 train loss:0.002 acc:100.0000 | val loss:0.920 acc:82.8000\n",
      "Epoch:0065 train loss:0.002 acc:100.0000 | val loss:0.897 acc:83.2000\n",
      "Epoch:0066 train loss:0.003 acc:100.0000 | val loss:0.877 acc:83.4000\n",
      "Epoch:0067 train loss:0.002 acc:100.0000 | val loss:0.858 acc:83.4000\n",
      "Epoch:0068 train loss:0.003 acc:100.0000 | val loss:0.844 acc:83.2000\n",
      "Epoch:0069 train loss:0.004 acc:100.0000 | val loss:0.835 acc:82.8000\n",
      "Epoch:0070 train loss:0.004 acc:100.0000 | val loss:0.832 acc:82.6000\n",
      "Epoch:0071 train loss:0.004 acc:100.0000 | val loss:0.830 acc:82.8000\n",
      "Epoch:0072 train loss:0.004 acc:100.0000 | val loss:0.834 acc:82.0000\n",
      "Epoch:0073 train loss:0.005 acc:100.0000 | val loss:0.834 acc:81.4000\n",
      "Epoch:0074 train loss:0.004 acc:100.0000 | val loss:0.830 acc:81.4000\n",
      "Epoch:0075 train loss:0.005 acc:100.0000 | val loss:0.821 acc:81.4000\n",
      "Epoch:0076 train loss:0.004 acc:100.0000 | val loss:0.808 acc:82.0000\n",
      "Epoch:0077 train loss:0.005 acc:100.0000 | val loss:0.797 acc:82.4000\n",
      "Epoch:0078 train loss:0.005 acc:100.0000 | val loss:0.789 acc:82.8000\n",
      "Epoch:0079 train loss:0.005 acc:100.0000 | val loss:0.784 acc:83.2000\n",
      "Epoch:0080 train loss:0.007 acc:100.0000 | val loss:0.778 acc:83.0000\n",
      "Epoch:0081 train loss:0.004 acc:100.0000 | val loss:0.774 acc:82.6000\n",
      "Epoch:0082 train loss:0.006 acc:100.0000 | val loss:0.773 acc:82.8000\n",
      "Epoch:0083 train loss:0.007 acc:100.0000 | val loss:0.777 acc:82.6000\n",
      "Epoch:0084 train loss:0.007 acc:100.0000 | val loss:0.788 acc:83.2000\n",
      "Epoch:0085 train loss:0.006 acc:100.0000 | val loss:0.803 acc:82.4000\n",
      "Epoch:0086 train loss:0.004 acc:100.0000 | val loss:0.819 acc:81.8000\n",
      "Test accuarcy:  0.8100000023841858\n",
      "Epoch:0001 train loss:2.698 acc:35.8333 | val loss:5.565 acc:41.2000\n",
      "Epoch:0002 train loss:12.287 acc:42.5000 | val loss:1.596 acc:38.4000\n",
      "Epoch:0003 train loss:1.463 acc:37.5000 | val loss:4.895 acc:49.8000\n",
      "Epoch:0004 train loss:5.835 acc:57.5000 | val loss:5.569 acc:23.8000\n",
      "Epoch:0005 train loss:7.203 acc:35.0000 | val loss:2.442 acc:41.2000\n",
      "Epoch:0006 train loss:6.418 acc:40.8333 | val loss:1.367 acc:51.2000\n",
      "Epoch:0007 train loss:1.054 acc:54.1667 | val loss:1.241 acc:42.0000\n",
      "Epoch:0008 train loss:0.780 acc:46.6667 | val loss:4.437 acc:50.8000\n",
      "Epoch:0009 train loss:4.917 acc:55.8333 | val loss:1.283 acc:42.2000\n",
      "Epoch:0010 train loss:6.623 acc:47.5000 | val loss:0.674 acc:48.2000\n",
      "Epoch:0011 train loss:0.348 acc:65.8333 | val loss:0.812 acc:41.8000\n",
      "Epoch:0012 train loss:0.378 acc:52.5000 | val loss:0.743 acc:45.4000\n",
      "Epoch:0013 train loss:0.346 acc:59.1667 | val loss:0.544 acc:62.0000\n",
      "Epoch:0014 train loss:0.250 acc:81.6667 | val loss:0.597 acc:71.0000\n",
      "Epoch:0015 train loss:0.301 acc:89.1667 | val loss:0.596 acc:69.0000\n",
      "Epoch:0016 train loss:0.278 acc:89.1667 | val loss:0.471 acc:68.2000\n",
      "Epoch:0017 train loss:0.184 acc:87.5000 | val loss:0.510 acc:66.6000\n",
      "Epoch:0018 train loss:0.184 acc:84.1667 | val loss:0.571 acc:66.0000\n",
      "Epoch:0019 train loss:0.196 acc:85.0000 | val loss:0.653 acc:65.0000\n",
      "Epoch:0020 train loss:0.230 acc:87.5000 | val loss:9.913 acc:48.0000\n",
      "Epoch:0021 train loss:7.627 acc:57.5000 | val loss:0.615 acc:59.0000\n",
      "Epoch:0022 train loss:0.254 acc:73.3333 | val loss:0.644 acc:50.4000\n",
      "Epoch:0023 train loss:0.390 acc:60.0000 | val loss:0.795 acc:51.4000\n",
      "Epoch:0024 train loss:0.652 acc:60.0000 | val loss:4.822 acc:43.6000\n",
      "Epoch:0025 train loss:5.061 acc:49.1667 | val loss:1.394 acc:46.0000\n",
      "Epoch:0026 train loss:1.271 acc:55.0000 | val loss:40.752 acc:20.4000\n",
      "Epoch:0027 train loss:187.794 acc:25.0000 | val loss:1.982 acc:48.2000\n",
      "Epoch:0028 train loss:2.192 acc:39.1667 | val loss:11.317 acc:47.4000\n",
      "Epoch:0029 train loss:30.238 acc:35.0000 | val loss:13.155 acc:38.4000\n",
      "Epoch:0030 train loss:14.769 acc:36.6667 | val loss:4.573 acc:49.2000\n",
      "Epoch:0031 train loss:5.939 acc:44.1667 | val loss:5.263 acc:41.6000\n",
      "Epoch:0032 train loss:2.404 acc:44.1667 | val loss:2.299 acc:38.4000\n",
      "Epoch:0033 train loss:10.200 acc:35.0000 | val loss:26.618 acc:20.4000\n",
      "Epoch:0034 train loss:106.190 acc:27.5000 | val loss:10.203 acc:20.4000\n",
      "Test accuarcy:  0.7149999737739563\n"
     ]
    }
   ],
   "source": [
    "for nlayer in [2, 4, 6, 8, 16, 32, 64]:\n",
    "#for nlayer in [3]:\n",
    "    for loss in [\"nll_loss\", \"focal_loss\"]:\n",
    "        if loss == \"focal_loss\":\n",
    "            penalty = True\n",
    "        else:\n",
    "            penalty = False\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        t_total = time.time()\n",
    "        bad_counter = 0\n",
    "        # best = 999999999\n",
    "        best = 0\n",
    "        # best_epoch = 0\n",
    "        acc = 0\n",
    "        # initacial\n",
    "        model = APPNP(nfeat=tensor_x.shape[1], nclass=int(tensor_y.max()) + 1,\n",
    "                      nlayers=nlayer, nhidden=nhidden, dropout=dropout).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               weight_decay=WEIGHT_DACAY)\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc = train(model, optimizer, loss, tensor_x, tensor_adjacency,\n",
    "                                          train_mask, tensor_y, kl_point, alpha=a, w1=w1,\n",
    "                                          gamma=gamma, penalty=penalty, w_gap=1)  # 计算当前模型训练集上的准确率\n",
    "            val_loss, val_acc = validate(model, loss, tensor_x, tensor_adjacency,\n",
    "                                         val_mask, tensor_y, kl_point, alpha=a, w1=w1,\n",
    "                                         gamma=gamma, penalty=penalty, w_gap=1)  # 计算当前模型在验证集上的准确率\n",
    "            # 记录训练过程中损失值和准确率的变化，用于画图\n",
    "\n",
    "            loss_history.append((train_loss, val_loss))\n",
    "            acc_history.append((train_acc, val_acc))\n",
    "\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                print('Epoch:{:04d}'.format(epoch + 1),\n",
    "                      'train',\n",
    "                      'loss:{:.3f}'.format(train_loss),\n",
    "                      'acc:{:.4f}'.format(train_acc * 100),\n",
    "                      '| val',\n",
    "                      'loss:{:.3f}'.format(val_loss),\n",
    "                      'acc:{:.4f}'.format(val_acc * 100))\n",
    "            if val_acc > best:\n",
    "                best = val_acc\n",
    "                # best_epoch = epoch\n",
    "                # acc = val_acc\n",
    "                torch.save(model.state_dict(), checkpt_file)\n",
    "                bad_counter = 0\n",
    "            else:\n",
    "                bad_counter += 1\n",
    "            if bad_counter == patience:\n",
    "                break\n",
    "\n",
    "        best_model = checkpt_file\n",
    "        test_acc, test_logits, test_label, x_repr = test(model, best_model, tensor_x, tensor_adjacency, test_mask, tensor_y, alpha=a)\n",
    "        #_, corrcoef = graph_smooth_metric(test_dic, x_repr[-1])\n",
    "        print(\"Test accuarcy: \", test_acc.item())\n",
    "        with open('/Users/MC/Desktop/research/GraphNeuralNetwork-master/chapter5/final_log.txt', 'a+') as f:\n",
    "            print('RESULT:------data:{}, layer:{}, hidden:{}, loss: {}------'\n",
    "                  .format(data, nlayer, nhidden, loss), file=f)\n",
    "            print(\"Test accuarcy:{}\".format(test_acc.item()), file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in x_repr:\n",
    "    _, corrcoef = graph_smooth_metric(test_dic, i)\n",
    "    print(corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_with_acc(loss_history, acc_history)\n",
    "#plt.savefig('pic/gcnii_64_64_focal.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_with_acc(loss_history, acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gg = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nodes_ = list(range(10))\n",
    "gg.add_nodes_from(nodes_)\n",
    "edges_ = [(0, 1), (0, 2), (0, 3), (0, 4), (5, 6), (5, 7), (5, 8), (5, 9), (0, 5)]\n",
    "gg.add_edges_from(edges_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subax1 = plt.subplot(121)\n",
    "nx.draw(gg, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adj_gg = np.array(nx.to_numpy_matrix(gg))\n",
    "# adj_gg\n",
    "x = np.eye(10, 10)\n",
    "y = np.array([0]*5 +[1]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_1 = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "idx_2 = [4, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 全部转化为tensor\n",
    "from utils import sparse_mx_to_torch_sparse_tensor\n",
    "from utils import sys_normalized_adjacency\n",
    "adj_tensor = sparse_mx_to_torch_sparse_tensor(sys_normalized_adjacency(adj_gg))\n",
    "x_tensor = torch.FloatTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "t_mask = torch.BoolTensor(sample_mask(idx_1, len(y)))\n",
    "v_mask = torch.BoolTensor(sample_mask(idx_2, len(y)))\n",
    "e_mask = torch.BoolTensor(sample_mask(list(range(10)), len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from Model import GCNI\n",
    "# for nlayers in [4, 8, 16, 32, 64]:\n",
    "data = \"test\"\n",
    "for nlayers in [16]:\n",
    "    for loss in [\"nll_loss\"]:\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        t_total = time.time()\n",
    "        bad_counter = 0\n",
    "        # best = 999999999\n",
    "        best = 0\n",
    "        # best_epoch = 0\n",
    "        acc = 0\n",
    "        # 模型定义：Model, Loss, Optimizer\n",
    "        \n",
    "        model = GCNI(nfeat=x_tensor.shape[1], nclass=int(y_tensor.max()) + 1, nlayers=nlayers, nhidden=nhidden,dropout=dropout).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               weight_decay=WEIGHT_DACAY)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            \n",
    "            # 记录训练过程中损失值和准确率的变化，用于画图\n",
    "            train_loss, train_acc = train_GCNII(model, optimizer, x_tensor, adj_tensor,\n",
    "                                          t_mask, y_tensor, loss)  # 计算当前模型训练集上的准确率\n",
    "            val_loss, val_acc = val_GCNII(model, x_tensor, adj_tensor,\n",
    "                                         v_mask, y_tensor, loss)  # 计算当前模型在验证集上的准确率\n",
    "            loss_history.append((train_loss, val_loss))\n",
    "            acc_history.append((train_acc, val_acc))\n",
    "\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                print('Epoch:{:04d}'.format(epoch + 1),\n",
    "                      'train',\n",
    "                      'loss:{:.3f}'.format(train_loss),\n",
    "                      'acc:{:.4f}'.format(train_acc * 100),\n",
    "                      '| val',\n",
    "                      'loss:{:.3f}'.format(val_loss),\n",
    "                      'acc:{:.4f}'.format(val_acc * 100))\n",
    "            if val_acc > best:\n",
    "                best = val_acc\n",
    "                # best_epoch = epoch\n",
    "                # acc = val_acc\n",
    "                torch.save(model.state_dict(), checkpt_file)\n",
    "                bad_counter = 0\n",
    "            else:\n",
    "                bad_counter += 1\n",
    "            if bad_counter == patience:\n",
    "                break\n",
    "best_model = checkpt_file\n",
    "test_acc, test_logits, test_label, x_repr = test_GCNII(model, best_model, x_tensor, adj_tensor, e_mask, y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from utils import avg_abs\n",
    "def avg_abs(matrix):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        matrix:\n",
    "\n",
    "    Returns:各节点的平均相关系数\n",
    "\n",
    "    \"\"\"\n",
    "    a = matrix.shape[0]\n",
    "    b = matrix.shape[1]\n",
    "    ss = np.zeros(a)\n",
    "    for i in range(a):\n",
    "        s = 0\n",
    "        for j in range(b):\n",
    "            s += abs(matrix[i][j])\n",
    "        ss[i] = (s - 1) / (a - 1)\n",
    "    return ss\n",
    "\n",
    "def GSI(node_embedding):\n",
    "    cor_matrix = np.corrcoef(node_embedding)\n",
    "    avg_cor_list = avg_abs(cor_matrix)\n",
    "    corrcoef = sum(avg_cor_list) / len(avg_cor_list)\n",
    "    return corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in x_repr:\n",
    "    corrcoef = GSI(i.numpy())\n",
    "    print(corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## kl指数与准确率之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_result_df(kl_point, label, logits, idy, pic_name, is_plot=False):\n",
    "    \"\"\"建立关于节点标签，预测值与kl值的数据框，方便进行数据分析\n",
    "    Args:\n",
    "        graph:nx中的graph对象\n",
    "        label:\n",
    "        logits:最后的输出\n",
    "        idy:用来计算的节点的index\n",
    "        pic_name:将图片进行存储的名称\n",
    "\n",
    "    Returns: dataframe（方便进行数据分析）\n",
    "    \"\"\"\n",
    "    # 转array数组\n",
    "    test_y = label[idy].numpy()  # 标签\n",
    "    prediction = logits.max(1)[1]\n",
    "    predict_y = prediction.numpy()  # 预测值\n",
    "    result_array = np.vstack((test_y, predict_y))  # 垂直堆积数组\n",
    "\n",
    "    result_df = pd.DataFrame(result_array)\n",
    "    result_df = result_df.T\n",
    "    result_df = result_df.rename(columns={0: \"label\", 1: \"prediction\"})\n",
    "    # 增加kl指数列\n",
    "    point = kl_point[idy]\n",
    "    result_df['kl_point'] = point\n",
    "    # 增加准确率列\n",
    "    result_df['acc'] = result_df.apply(lambda x: 1 if x.label == x.prediction else 0, axis=1)\n",
    "    # 将kl值分箱离散处理\n",
    "    bins = [0, 1e-5, 5, 10, 15, 20, 25, 30, 40]\n",
    "    labels = [(bins[i], bins[i + 1]) for i in range(len(bins) - 1)]\n",
    "    result_df['interval'] = pd.cut(x=result_df['kl_point'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "    if is_plot:\n",
    "        acc_dis = result_df.groupby(['interval'])['acc'].mean()\n",
    "        acc = ['{}'.format(i) for i in acc_dis.index]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(acc, list(acc_dis.values), color='#2F4F4F')\n",
    "        plt.xlabel('CDS', size=13)\n",
    "        plt.ylabel('accuracy', size=13)\n",
    "        plt.savefig('pic/{}.jpg'.format(pic_name))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = build_result_df(kl_point, tensor_y, test_logits, idy=test_mask, pic_name=\"**.jpg\", is_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import avg_abs\n",
    "def graph_smooth_metric(dic, node_embedding):\n",
    "    \"\"\"\n",
    "    计算全图的相关系数\n",
    "    Args:\n",
    "        dic: 存储多次放回抽样的样本\n",
    "        node_embedding: 所有节点表示\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # node_embedding = node_embedding.numpy()\n",
    "    # 记录样本均值\n",
    "    corrcoef = []\n",
    "    for i in range(len(dic)):\n",
    "        sample = dic[i]\n",
    "        use_embedding = node_embedding[sample]\n",
    "        # 抽样节点的相关系数矩阵\n",
    "        cor_matrix = np.corrcoef(use_embedding)\n",
    "        avg_cor_list = avg_abs(cor_matrix)\n",
    "        corrcoef.append(sum(avg_cor_list) / len(avg_cor_list))\n",
    "        # abs_matrix = np.maximum(x, -x), 取绝对值运算\n",
    "    return corrcoef, sum(corrcoef) / len(corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "sns.lineplot(lay_l, result_1, label='all', linestyle='--')\n",
    "sns.lineplot(lay_l, result_30, label='sample_30')\n",
    "sns.lineplot(lay_l, result_50, label='sample_50')\n",
    "sns.lineplot(lay_l, result_100, label='sample_100')\n",
    "sns.lineplot(lay_l, result_150, label='sample_150')\n",
    "sns.lineplot(lay_l, result_200, label='sample_200')\n",
    "plt.xlabel('layer')\n",
    "plt.ylabel(\"GSI\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#lay_l = [0, 2, 4, 6, 8, 10, 12, 14, 16]\n",
    "#plt.plot(lay_l, result_1, label='new_focal_loss')\n",
    "plt.plot(lay_l, result_2, label='cross_loss')\n",
    "plt.xlabel('layer')\n",
    "plt.ylabel(\"smooth_point\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"pic/gcnii_32_cp.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(result_1, result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[0.602859555520298, 0.9566081323551006, 0.9844643359084218, 0.9907878088419304, 0.9922291004721646, 0.9949655117537931, 0.9946713176155001, 0.9941030748387849, 0.9944954783924311]\n",
    "\n",
    "[0.3108020890582047, 0.8576967839466505, 0.9422434068356622, 0.9719488794943217, 0.9887902997788125, 0.9931819256177867, 0.9948218507600602, 0.996486339799523, 0.9964306877167679]\n",
    "\n",
    "[0.42299069500722747, 0.5165361688924619, 0.529959208265846, 0.5198351359427967, 0.5163483046298408, 0.5164835281020107, 0.516444828882187, 0.5164367938621377, 0.5188479606399918]\n",
    "\n",
    "[0.3395749384658731, 0.43567608963945287, 0.45004840274331687, 0.43373123438413097, 0.42559468333665434, 0.4190755935974977, 0.41374638790468027, 0.41146850683486785, 0.4110902065272529]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 实验结果记录\n",
    "0-16：80.70%，\n",
    "0-64：81.4%\n",
    "1-64：81.60%-81.70%\n",
    "\n",
    "0-128:81.70%\n",
    "0.813400000333786\n",
    "（0.8149999976158142，0.8100000023841858）\n",
    "\n",
    "2-64:80.8%\n",
    "4-64:76.8%\n",
    "8-64:53.4%\n",
    "\n",
    "1-128:84.40% /83.0%,这种情况下好不稳定。\n",
    "82.2%\n",
    "(0.8389999866485596 0.8019999861717224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
